{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a203cf",
   "metadata": {},
   "source": [
    "\n",
    "# Smart Retail Navigator: Unifying RAG, LLM, and Annoy for Advanced Query Intelligence\n",
    "\n",
    "This notebook presents an enhanced analysis using a Structured Retrieval-Augmented Generation (RAG) System, specifically tailored for the retail sector. The system leverages advanced data processing techniques and machine learning models to provide comprehensive insights into retail operations, customer behavior, and sales performance. Through detailed examples and explanations, we aim to demonstrate the application of cutting-edge AI technologies in transforming retail analytics and decision-making processes. The below diagram shows overall architecture:\n",
    "\n",
    "![Architecture](architecture.png)\n",
    "\n",
    "The key aspects of the Smart Retail Navigator system architecture:\n",
    "\n",
    "- **Data Layer**: Manages storage and access of retail data \n",
    "\n",
    "- **Retrieval-Augmented Generation (RAG)**: Retrieves relevant data and generates query responses by combining information retrieval and deep learning\n",
    "\n",
    "- **Large Language Models (LLM)**: Understand queries and generate human-like responses after specialized fine-tuning \n",
    "\n",
    "- **Annoy**: Rapidly retrieves most relevant information for queries via similarity searches in vector spaces\n",
    "\n",
    "- **Query Processor**: Coordinates overall workflow - query understanding by LLM, data retrieval via Annoy and RAG, and response generation\n",
    "\n",
    "- **Analytics Module**: Transforms system outputs into business insights for data-driven decision making\n",
    "\n",
    "The architecture strategically integrates the latest innovations in AI to ensure scalability, efficiency, accuracy and cutting-edge capabilities for enabling advanced retail analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e621bb9",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation and Exploration\n",
    "\n",
    "In this section, we delve into the initial steps of our analysis: preparing and exploring the dataset. Our focus is on understanding the characteristics of the data, identifying patterns, and preparing it for further analysis. We'll cover data loading, cleaning, and basic exploratory data analysis (EDA) techniques that are crucial for any data science project.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b13876",
   "metadata": {},
   "source": [
    "\n",
    "## Predictive Modeling and Analysis\n",
    "\n",
    "Following data preparation, we transition to the core of our analysisâ€”predictive modeling. This section explores the creation and evaluation of models that predict future retail trends based on historical data. We'll discuss model selection, training, and validation, emphasizing the importance of accuracy and reliability in predictions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c87bd",
   "metadata": {},
   "source": [
    "\n",
    "## Insights and Conclusion\n",
    "\n",
    "In the final section, we synthesize our findings into actionable insights. Drawing from the data exploration and predictive modeling phases, we outline key takeaways and recommend strategies for retail businesses to optimize their operations, enhance customer satisfaction, and boost sales. This comprehensive analysis demonstrates the transformative potential of AI in retail.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a15f538ea5c4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:50:54.269122Z",
     "start_time": "2024-02-19T14:50:54.265727Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install transformers annoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326c255dbc81696",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Retrieval:** The similarity between a query \\(q\\) and a document \\(d\\) is often computed using cosine similarity:\n",
    "  \\[ S(d, q) = \\frac{d \\cdot q}{\\|d\\| \\|q\\|} \\]\n",
    "- **Generation:** The probability of generating a word \\(w_t\\) given the context \\(C\\) and previous words \\(w_{<t}\\) is modeled as:\n",
    "  \\[ P(w_t | w_{<t}, C) = \\text{softmax}(W_h h_t) \\]\n",
    "\n",
    "### Large Language Models (LLM)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Self-Attention:** The self-attention mechanism's computation is defined as:\n",
    "  \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
    "\n",
    "### LangChain\n",
    "\n",
    "**Core Concepts:**\n",
    "LangChain, developed as an open-source toolkit, simplifies the creation of applications leveraging LLMs by facilitating integration with external computation and data sources. Its components include:\n",
    "\n",
    "- **Schema:** Defines core data structures like Text, ChatMessages, Examples, and Document, essential for interacting with language models.\n",
    "- **Models:** Categorizes into Language Models, Chat Models, and Text Embedding Models, offering interfaces for seamless integration with LLMs.\n",
    "- **Prompts:** Crafting prompts is vital for directing LLMs. LangChain introduces PromptTemplates for constructing prompts dynamically.\n",
    "- **Indexes:** Serve as bridges between documents/data and LLMs, crucial for enriching models with context-specific information.\n",
    "- **Memory:** Facilitates storing and retrieving chat history or conversational context, enhancing the model's ability to produce coherent and context-aware responses.\n",
    "- **Chains:** Enable the sequencing of multiple components, such as data retrieval, prompt generation, and response parsing, into a cohesive workflow.\n",
    "- **Agents:** Utilize LLMs for selecting and sequencing actions, embodying the decision-making prowess of LLMs in application scenarios.\n",
    "\n",
    "**Innovations in LangChain:**\n",
    "LangChain's modular abstractions for these components significantly lower the barrier to integrating complex LLM functionalities into applications. By abstracting the complexities involved in handling language models, data retrieval, and processing, LangChain empowers developers to build more sophisticated, context-aware applications with ease.\n",
    "\n",
    "**Mathematical Extensions:**\n",
    "While LangChain primarily focuses on the architectural and integration aspects of using LLMs, the underlying mathematical principles of its components (especially Models and Indexes) are grounded in the computations of neural networks, vector space models, and embeddings. For instance, the Text Embedding Models convert textual data into numerical vectors, capturing semantic meanings in a high-dimensional space, which can be mathematically represented as:\n",
    "\\[ \\text{Embedding}(text) = V \\]\n",
    "where \\(V\\) is a vector representing the text in a semantic vector space.\n",
    "\n",
    "### Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Tree Construction:** Annoy uses random projection trees for partitioning data, optimizing for both speed and memory efficiency.\n",
    "- **Approximate Search:** The search in Annoy is approximated to quickly retrieve near neighbors without exhaustive search, significantly reducing query time.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "This comprehensive overview, enriched with details from the LangChain components guide and mathematical principles, offers a deeper understanding of the technologies and methodologies driving today's AI applications. LangChain, by abstracting the complexity of integrating LLMs with external data and computational resources, stands out as a pivotal framework for developers aiming to leverage the power of language models in their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a73509e88c59e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mock Data Generation\n",
    "\n",
    "The provided Python function `generate_retail_mock_data` is designed to create mock retail product descriptions. It's a versatile tool for generating a dataset that could be used in various retail or e-commerce data analysis, machine learning models, or simply for testing and demonstration purposes. Here's a detailed breakdown of how this function works and its components:\n",
    "\n",
    "### Function Definition and Parameters\n",
    "- **Function Name:** `generate_retail_mock_data`\n",
    "- **Parameters:**\n",
    "  - `categories`: A list of strings representing product categories. Default categories are 'shoes', 'apparel', and 'jackets'.\n",
    "  - `num_items_per_category`: An integer that defines how many items to generate per category. The default is 5000.\n",
    "  - `seed`: An integer used to initialize the random number generator. This ensures that the function generates the same sequence of descriptions each time it runs with the same seed value. The default is 42.\n",
    "\n",
    "### Core Functionality\n",
    "1. **Setting the Random Seed:** By setting the random seed using `random.seed(seed)`, the function ensures reproducibility. This means that every time the function is called with the same seed, it will produce the same set of mock product descriptions.\n",
    "\n",
    "2. **Generating Product Descriptions:** The function iterates over each category provided in the `categories` list. For each category, it generates `num_items_per_category` product descriptions.\n",
    "\n",
    "3. **Description Format:** Each product description is formatted as a string that includes:\n",
    "   - The category name, capitalized.\n",
    "   - A sequential number (1 to `num_items_per_category` for each category) to uniquely identify each item within its category.\n",
    "   - A combination of three randomly selected features from the list `['lightweight', 'durable', 'waterproof', 'stylish', 'comfortable']`. This randomness in feature selection adds variety to the mock data.\n",
    "\n",
    "4. **Appending Descriptions:** Each generated description is appended to the `product_descriptions` list.\n",
    "\n",
    "### Output\n",
    "- The function returns the `product_descriptions` list, containing all the generated descriptions.\n",
    "\n",
    "### Example Output\n",
    "The last line of the provided code prints the first five product descriptions from the generated mock data. Here's what an example output might look like:\n",
    "```\n",
    "Shoes 1 with features: durable stylish comfortable in category shoes.\n",
    "Shoes 2 with features: lightweight waterproof durable in category shoes.\n",
    "Shoes 3 with features: comfortable stylish waterproof in category shoes.\n",
    "Shoes 4 with features: durable waterproof stylish in category shoes.\n",
    "Shoes 5 with features: lightweight comfortable stylish in category shoes.\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "This mock data generation function can be particularly useful in scenarios where real product data is unavailable for testing, such as:\n",
    "- **Development and Testing:** During the development of e-commerce platforms, where testing data retrieval, search functionality, and recommendation systems are necessary.\n",
    "- **Data Analysis Projects:** For educational purposes or demonstration of data analysis techniques specific to retail data.\n",
    "- **Machine Learning Models:** To train or test machine learning models that require product description data, before applying the model to real datasets.\n",
    "\n",
    "### Customization and Extension\n",
    "Users can easily customize the function to fit specific requirements by:\n",
    "- Adding or changing the product categories.\n",
    "- Modifying the number of items per category.\n",
    "- Changing the list of features to include other attributes relevant to their products.\n",
    "- Adjusting the seed for different variations of mock data.\n",
    "\n",
    "This function exemplifies a simple yet powerful method for generating structured, varied, and reproducible mock data, which can be adapted for a wide range of applications in retail and e-commerce domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4263dd944f97f76d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:50:54.830289Z",
     "start_time": "2024-02-19T14:50:54.805057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoes 1 with features: lightweight comfortable waterproof in category shoes.\n",
      "Shoes 2 with features: waterproof durable lightweight in category shoes.\n",
      "Shoes 3 with features: durable lightweight waterproof in category shoes.\n",
      "Shoes 4 with features: comfortable lightweight waterproof in category shoes.\n",
      "Shoes 5 with features: stylish lightweight comfortable in category shoes.\n",
      "Shoes 6 with features: lightweight durable comfortable in category shoes.\n",
      "Shoes 7 with features: comfortable lightweight waterproof in category shoes.\n",
      "Shoes 8 with features: durable stylish lightweight in category shoes.\n",
      "Shoes 9 with features: stylish waterproof lightweight in category shoes.\n",
      "Shoes 10 with features: durable stylish comfortable in category shoes.\n",
      "Shoes 11 with features: waterproof durable lightweight in category shoes.\n",
      "Shoes 12 with features: waterproof lightweight stylish in category shoes.\n",
      "Shoes 13 with features: stylish lightweight durable in category shoes.\n",
      "Shoes 14 with features: waterproof comfortable lightweight in category shoes.\n",
      "Shoes 15 with features: stylish lightweight durable in category shoes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated function to generate mock retail product data with import\n",
    "import random\n",
    "\n",
    "def generate_retail_mock_data(categories=['shoes', 'apparel', 'jackets'], num_items_per_category=5000, seed=42):\n",
    "    random.seed(seed)\n",
    "    product_descriptions = []\n",
    "    for category in categories:\n",
    "        for i in range(num_items_per_category):\n",
    "            description = f\"{category.capitalize()} {i+1} with features: \" +                           \" \".join(random.sample(['lightweight', 'durable', 'waterproof', 'stylish', 'comfortable'], 3)) +                           f\" in category {category}.\"\n",
    "            product_descriptions.append(description)\n",
    "    return product_descriptions\n",
    "\n",
    "mock_retail_product_descriptions = generate_retail_mock_data()\n",
    "num_products = len(mock_retail_product_descriptions)\n",
    "print(\"\\n\".join(mock_retail_product_descriptions[:15]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b3ec1860473171",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:50:55.354066Z",
     "start_time": "2024-02-19T14:50:54.973323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy index is built with 15000 items.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "vector_length = 100  # Assuming a 100-dimensional vector for each product description\n",
    "annoy_index = AnnoyIndex(vector_length, 'angular')\n",
    "\n",
    "# Mock function to convert descriptions to vectors\n",
    "def description_to_vector(description):\n",
    "    np.random.seed(hash(description) % (2**32 - 1))\n",
    "    return np.random.rand(vector_length)\n",
    "\n",
    "# Adding items to Annoy index\n",
    "for i, description in enumerate(mock_retail_product_descriptions):\n",
    "    vec = description_to_vector(description)\n",
    "    annoy_index.add_item(i, vec)\n",
    "\n",
    "annoy_index.build(10)  # Using 10 trees\n",
    "print(\"Annoy index is built with\", num_products, \"items.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60101a8acb5c7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
    "\n",
    "Annoy is a C++ library with Python bindings designed to efficiently search for points in space that are close to a given query point, emphasizing speed and minimal memory usage while accepting a trade-off in precision. It's particularly useful for implementing recommendation systems, enhancing search engines, and facilitating various machine learning applications where quick nearest neighbor queries are essential.\n",
    "\n",
    "### Key Features:\n",
    "- **Memory-efficient Indexing:** Annoy creates large, read-only, file-based data structures that are memory-mapped, allowing multiple processes to share the same data without duplicating it in memory.\n",
    "- **Incremental Updates:** It supports adding items to the index incrementally without the need to rebuild the index from scratch, making it suitable for dynamic datasets.\n",
    "- **Persistence:** Annoy indexes can be saved to disk and later reloaded, facilitating persistent storage and retrieval of vector data across sessions.\n",
    "\n",
    "### Usage in This Notebook:\n",
    "In the context of this notebook, Annoy is utilized to store and query embeddings of retail product descriptions. Here's how we integrate Annoy for a practical use-case:\n",
    "\n",
    "1. **Embedding Storage:** We generate embeddings for each product description using a mock function that simulates the conversion of textual descriptions into 100-dimensional vectors. These embeddings are stored in an Annoy index.\n",
    "2. **Index Creation:** An AnnoyIndex instance is created with the specified vector length and metric ('angular' for cosine similarity). Each product's embedding vector is added to the index, which is then built with a specified number of trees to optimize query performance.\n",
    "3. **Querying:** For a given query, its embedding is calculated and used to fetch the nearest neighbors from the Annoy index. This process efficiently identifies the most relevant product descriptions based on the similarity of their embeddings to the query.\n",
    "\n",
    "This demonstration showcases the use of Annoy as a vector database for embedding-based retrieval, illustrating its application in scenarios where finding similar items quickly is crucial, such as in retail product recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29016635cb7d53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Integration of LangChain with RAG and Annoy\n",
    "\n",
    "In this demonstration, we explore the integration of LangChain with Retrieval-Augmented Generation (RAG) and Annoy, showcasing a seamless workflow that combines the retrieval capabilities of Annoy with the generative prowess of a Large Language Model (LLM) provided by Hugging Face's `transformers`. This integration exemplifies how to leverage the strengths of both retrieval and generation to enhance the relevance and richness of generated text based on a given query.\n",
    "\n",
    "### Workflow Overview:\n",
    "1. **Query Embedding:** For a given query, we first convert it into an embedding vector using a mock function `description_to_vector`. This function simulates the process of transforming textual data into a numerical representation that can be processed by machine learning models.\n",
    "\n",
    "2. **Retrieval with Annoy:** Using the query's vector representation, we retrieve the nearest neighbor product descriptions from an Annoy index. Annoy efficiently identifies the vectors in the dataset that are closest to the query vector, based on cosine similarity. This step highlights Annoy's utility in quickly fetching relevant data from a large dataset.\n",
    "\n",
    "3. **Context Assembly:** The retrieved product descriptions are concatenated to form a context string. This assembled context is then used to inform the generation process, ensuring that the generated text is relevant to the specifics of the query.\n",
    "\n",
    "4. **Text Generation with LangChain and RAG:** The concatenated context is fed into a generative model from the `transformers` library, specifically `distilgpt2`, alongside the original query. This generative step uses the context to produce a response that is not only contextually aware but also creatively enriched by the language model.\n",
    "\n",
    "5. **HTML Presentation:** The query, retrieved context, and generated response are presented in an HTML format for enhanced readability. This step demonstrates how the integration of these technologies can be used to create user-friendly outputs for applications such as product recommendation systems or automated customer service responses.\n",
    "\n",
    "### Example Usage:\n",
    "In our example, we query for \"Looking for stylish and comfortable shoes.\" The process involves embedding the query, retrieving related product descriptions using Annoy, and generating a response that synthesizes the query intent with the retrieved information. The final output is displayed in a styled HTML block, making it easy to visualize the integration's effectiveness in producing relevant and engaging content.\n",
    "\n",
    "This integration exemplifies a practical application of combining retrieval and generative models to enhance the capabilities of AI-driven systems. By leveraging the specific strengths of Annoy for efficient data retrieval and the generative capabilities of language models like `distilgpt2`, developers can create sophisticated solutions that address complex queries with contextually rich and relevant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a1d6191e5a7116",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:51:08.221855Z",
     "start_time": "2024-02-19T14:50:55.499230Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n        <h2>Query</h2>\n        <p>Looking for stylish and comfortable shoes</p>\n        <h2>Retrieved Context</h2>\n        <p>Apparel 3024 with features: durable lightweight comfortable in category apparel. Jackets 2855 with features: stylish comfortable waterproof in category jackets. Jackets 4247 with features: lightweight durable waterproof in category jackets. Jackets 341 with features: waterproof durable lightweight in category jackets. Apparel 872 with features: lightweight comfortable waterproof in category apparel.</p>\n        <h2>Response</h2>\n        <p style=\"color: #27ae60;\">Query: Looking for stylish and comfortable shoes. Based on: Apparel 3024 with features: durable lightweight comfortable in category apparel. Jackets 2855 with features: stylish comfortable waterproof in category jackets. Jackets 4247 with features: lightweight durable waterproof in category jackets. Jackets 341 with features: waterproof durable lightweight in category jackets. Apparel 872 with features: lightweight comfortable waterproof in category apparel. Jackets 1852 with features: lightweight lightweight waterproof in category jackets. Jackets 1479 with features: lightweight waterproof in</p>\n    </div>\n    "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import HTML\n",
    "\n",
    "def simple_langchain_integration_v2(query):\n",
    "    generator = pipeline('text-generation', model='distilgpt2')\n",
    "    # Assuming description_to_vector and other necessary parts are defined elsewhere\n",
    "    query_vector = description_to_vector(query)\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(query_vector, 5)  # Assuming annoy_index is defined elsewhere\n",
    "    retrieved_docs = [mock_retail_product_descriptions[i] for i in nearest_ids]  # Assuming mock_retail_product_descriptions is defined elsewhere\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "    # Added truncation=True to avoid the warning\n",
    "    response = generator(f'Query: {query}. Based on: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)\n",
    "    generated_text = response[0]['generated_text']\n",
    "\n",
    "    # Create HTML content with CSS styling for better visual presentation\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "        <h2>Query</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Retrieved Context</h2>\n",
    "        <p>{retrieved_context}</p>\n",
    "        <h2>Response</h2>\n",
    "        <p style=\"color: #27ae60;\">{generated_text}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example use\n",
    "simple_langchain_integration_v2(\"Looking for stylish and comfortable shoes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266d5d8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive demonstration of integrating Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) from Hugging Face, leveraging Annoy for efficient nearest neighbor searches, all facilitated through the LangChain framework. The primary focus was on a retail context, showcasing how the amalgamation of retrieval systems and generative models can significantly enhance application capabilities in delivering relevant and contextually rich responses to user queries. Here's a summary of the workflow and key takeaways:\n",
    "\n",
    "- **What:** The notebook outlines the process of generating mock retail product descriptions, indexing these descriptions using Annoy for fast retrieval, and then integrating this with a generative model from Hugging Face (distilGPT-2) for text generation, all orchestrated using the LangChain framework.\n",
    "\n",
    "- **Why:** The integration aims to demonstrate the power of combining vector space retrieval with generative AI to improve the relevance and specificity of responses in a simulated retail query scenario. This approach exemplifies how businesses can leverage AI to enhance user experiences through personalized and context-aware interactions.\n",
    "\n",
    "- **How:**\n",
    "  - **Mock Data Generation:** We started by generating a dataset of mock product descriptions to simulate a retail inventory.\n",
    "  - **Annoy Indexing:** The descriptions were then converted into vector embeddings and indexed using Annoy, enabling efficient similarity-based retrieval.\n",
    "  - **LangChain Integration:** We showcased how LangChain can facilitate the integration of Annoy with a generative LLM to process user queries, retrieve contextually relevant product descriptions, and generate informative responses.\n",
    "  - **Text Generation and Display:** Utilizing the Hugging Face `pipeline` for text generation, the notebook demonstrated generating responses based on the context provided by Annoy's nearest neighbor search, with the output presented in an HTML format for enhanced readability.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Efficiency and Relevance:** The use of Annoy for embedding storage and retrieval showcases an efficient method to add contextual relevance to LLM-generated responses.\n",
    "- **Scalability:** This approach illustrates how scalable solutions can be built for real-world applications, accommodating large datasets typical in retail environments.\n",
    "- **Customizability:** The modular nature of LangChain, combined with the flexibility of Annoy and the generative power of LLMs, underscores the potential for customization according to specific application needs or domains.\n",
    "- **User Experience Enhancement:** The integration exemplifies how AI can be used to significantly enhance user experience, providing a foundation for developing advanced AI-driven retail applications.\n",
    "\n",
    "Through this integration, we've demonstrated a scalable and efficient method to bring context-awareness and relevance to AI-generated text, paving the way for innovative applications in retail and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0d60b19d9e54d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Detailed System Architecture Overview\n",
    "\n",
    "This section delves into the architecture of the system implemented in this notebook, leveraging the Mermaid syntax for a comprehensive visual depiction. We intricately integrate Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) from Hugging Face, employing the Annoy library for efficient similarity searches, all orchestrated within the LangChain framework to enhance retail query processing.\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "![RAG Sequence Diagram](rag_sequence_digram.png)\n",
    "\n",
    "### Architecture Components and Interactions\n",
    "\n",
    "- **User Query:** Initiates the process, where users enter their queries regarding retail products, starting the interaction flow.\n",
    "\n",
    "- **LangChain Framework:** Serves as the central orchestrator, efficiently managing the workflow from user query input to fetching nearest neighbors via Annoy and generating responses through LLM. It ensures seamless integration and communication between different components.\n",
    "\n",
    "- **Annoy Index:** A critical component that stores pre-processed vector embeddings of product descriptions, enabling quick and efficient retrieval of items similar to the user query.\n",
    "\n",
    "- **Retrieve Nearest Neighbors:** This step is crucial for identifying contextually relevant product descriptions based on the user's query, which are then utilized to inform the response generation process.\n",
    "\n",
    "- **LLM (Hugging Face):** At this stage, the system leverages a pre-trained language model to generate detailed and contextually enriched responses by incorporating both the original query and the context provided by the nearest neighbors.\n",
    "\n",
    "- **Generate Response:** The synthesis of input and retrieved context through the LLM culminates in the generation of a response tailored to the user's query, embodying the integration of RAG principles.\n",
    "\n",
    "- **Display Results:** The final step where the system presents the generated response back to the user, completing the cycle and providing a cohesive answer to the query.\n",
    "\n",
    "### Highlighted Features:\n",
    "\n",
    "- **Rapid Context Retrieval:** Utilizes Annoy for the swift retrieval of relevant context, significantly speeding up the response generation process.\n",
    "\n",
    "- **Scalable System Design:** Engineered to accommodate the extensive and growing datasets typical in retail, ensuring the system's scalability.\n",
    "\n",
    "- **Modular and Flexible:** The architecture's modular design promotes flexibility, allowing for the easy replacement or enhancement of individual components, such as swapping the LLM model or modifying the retrieval strategy.\n",
    "\n",
    "- **Enhanced User Engagement:** By delivering precise, context-aware responses, the system significantly improves user interaction and satisfaction, showcasing the potential of AI in transforming retail experiences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbf53842bc35dc0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:51:08.224861Z",
     "start_time": "2024-02-19T14:51:08.220969Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf7700d",
   "metadata": {},
   "source": [
    "# Enhancing E-commerce with NingLab eCeLLM-S\n",
    "\n",
    "The integration of NingLab's eCeLLM-S model into our retail analytics framework signifies a leap forward in applying advanced Large Language Models (LLMs) to the e-commerce domain. This section delves into the technical underpinnings, objectives, and implementation strategy of leveraging eCeLLM-S for enriching e-commerce content and customer interaction.\n",
    "\n",
    "## Background on LLMs and eCeLLM-S\n",
    "\n",
    "Large Language Models (LLMs) like GPT-3 have revolutionized the field of natural language processing (NLP) with their ability to generate coherent and contextually relevant text across a broad spectrum of applications. Building on this foundation, eCeLLM-S is designed to specifically address the nuances and complexities of e-commerce text generation, from product descriptions to customer engagement.\n",
    "\n",
    "### Core Technologies\n",
    "\n",
    "eCeLLM-S employs a transformer-based architecture, renowned for its self-attention mechanism, which allows the model to weigh the importance of different parts of the input text differently. This is crucial for generating text that is not only grammatically correct but also contextually aligned with the subject matter.\n",
    "\n",
    "#### Mathematical Foundation of Transformers\n",
    "\n",
    "The transformer model utilizes several key equations, including the self-attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "- $Q$, $K$, and $V$ represent the query, key, and value vectors, respectively.\n",
    "- $d_k$ is the dimensionality of the key vectors, providing a normalization factor.\n",
    "\n",
    "This formula allows the model to dynamically focus on different parts of the input sequence, enhancing the relevance and coherence of the generated text.\n",
    "\n",
    "### Objectives of eCeLLM-S Integration\n",
    "\n",
    "- **Contextual Relevance**: Generate text that reflects the specific context of retail products and customer interactions.\n",
    "- **Enhanced Descriptions**: Create detailed and engaging product descriptions that capture the essence and appeal of products.\n",
    "- **Customer Engagement**: Facilitate improved customer interaction through personalized and informative responses.\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "The effective deployment of eCeLLM-S within our retail analysis ecosystem involves a multi-faceted approach:\n",
    "\n",
    "1. **Data Curation**: Assemble a diverse dataset that encompasses a wide range of our retail products and customer feedback.\n",
    "2. **Model Fine-Tuning**: Adapt eCeLLM-S to our specific retail context to enhance its output's relevance and accuracy.\n",
    "3. **Workflow Integration**: Seamlessly incorporate eCeLLM-S-generated content into our product listings and customer service processes.\n",
    "4. **Iterative Refinement**: Continuously monitor and refine the model's performance based on user feedback and emerging retail trends.\n",
    "\n",
    "\n",
    "**The strategic integration of NingLab's eCeLLM-S model represents a forward-thinking approach to harnessing the power of LLMs in the retail sector. By leveraging its advanced text generation capabilities, we can significantly enhance the quality of our product descriptions and the effectiveness of our customer interactions, setting a new benchmark for AI-driven e-commerce excellence.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdddfc93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:51:53.643124Z",
     "start_time": "2024-02-19T14:51:08.226676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f472eb7de8d464fa07c568f0e9a43a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated product description:\n",
      "Generate a catchy product description for a pair of eco-friendly sneakers.\n",
      "Input: \n",
      "Output: Step into the future with these eco-friendly sneakers. Made from recycled materials and organic cotton, these sneakers are not only stylish and comfortable, but also sustainable and ethical. Whether you're running, walking, or dancing, these sneakers will keep you on your feet and on your mission to save the planet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the eCeLLM-S pipeline for text generation\n",
    "eCeLLM_S_pipeline = pipeline(\"text-generation\", model=\"NingLab/eCeLLM-S\")\n",
    "\n",
    "# Example usage with a prompt related to retail\n",
    "prompt = \"Generate a catchy product description for a pair of eco-friendly sneakers\"\n",
    "generated_text = eCeLLM_S_pipeline(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated product description:\")\n",
    "print(generated_text[0]['generated_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33273fd9",
   "metadata": {},
   "source": [
    "\n",
    "### Application in Retail Analysis\n",
    "\n",
    "The above example demonstrates the use of eCeLLM-S in generating a product description. Such capabilities can be extended to various aspects of retail analysis, including but not limited to, generating creative product names, detailed product descriptions, and personalized marketing messages.\n",
    "\n",
    "By integrating eCeLLM-S, we can enhance the customer experience through more engaging and informative content, contributing to improved customer satisfaction and sales.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90eee7",
   "metadata": {},
   "source": [
    "\n",
    "## Integration with Lanchain Using Vector RAG\n",
    "\n",
    "To further enhance our retail analytics capabilities, we integrate Lanchain with vector Retrieval-Augmented Generation (RAG) for advanced query understanding and information retrieval. This integration aims to leverage the synergies between Lanchain's blockchain technology and RAG's powerful retrieval capabilities to enhance data veracity and retrieval efficiency in our retail analysis.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- **Enhance Data Veracity**: Utilize Lanchain to ensure the integrity and veracity of the retail data used in our analysis.\n",
    "- **Improve Retrieval Efficiency**: Leverage vector RAG for efficient retrieval of relevant information from our extensive retail dataset.\n",
    "- **Innovate Retail Analytics**: Combine blockchain technology and state-of-the-art NLP to pioneer innovative retail analytics solutions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a38f316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:52:10.306848Z",
     "start_time": "2024-02-19T14:51:53.645140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cef389759674369a1689a4b7a64894d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n        <h2>Query</h2>\n        <p>Looking for stylish and comfortable jacket</p>\n        <h2>Retrieved Context</h2>\n        <p>Apparel 3033 with features: durable waterproof stylish in category apparel. Shoes 3831 with features: lightweight waterproof comfortable in category shoes. Apparel 4161 with features: comfortable waterproof stylish in category apparel. Apparel 4906 with features: comfortable durable lightweight in category apparel. Apparel 3529 with features: comfortable lightweight waterproof in category apparel.</p>\n        <h2>Generated Response</h2>\n        <p style=\"color: #27ae60;\">Query: Looking for stylish and comfortable jacket. Context: Apparel 3033 with features: durable waterproof stylish in category apparel. Shoes 3831 with features: lightweight waterproof comfortable in category shoes. Apparel 4161 with features: comfortable waterproof stylish in category apparel. Apparel 4906 with features: comfortable durable lightweight in category apparel. Apparel 3529 with features: comfortable lightweight waterproof in category apparel. Apparel 3287 with features: comfortable lightweight in category apparel. Apparel 4231 with features:</p>\n    </div>\n    "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Function to integrate language model with retail data\n",
    "def enhanced_langchain_integration(query):\n",
    "    # Using eCeLLM-S for e-commerce specific generation and DistilGPT-2 for general text\n",
    "    eceLLM_generator = pipeline('text-generation', model='NingLab/eCeLLM-S')  # Adjust the model ID as needed\n",
    "    general_generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "    # Convert query to vector (Assuming this function is defined and works with your data)\n",
    "    query_vector = description_to_vector(query)\n",
    "\n",
    "    # Retrieve similar product descriptions based on the query vector\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(query_vector, 5)  # Assuming annoy_index is properly set up\n",
    "    retrieved_docs = [mock_retail_product_descriptions[i] for i in nearest_ids]  # Assuming this variable holds your mock data\n",
    "\n",
    "    # Combine retrieved context for better generation\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "\n",
    "    # Generate response with both models and choose based on context\n",
    "    eceLLM_response = eceLLM_generator(f'Query: {query}. Based on: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
    "    general_response = general_generator(f'Query: {query}. Context: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
    "\n",
    "    # Choose response based on some criteria (e.g., length, relevance, etc.) - for simplicity, just using eCeLLM-S here\n",
    "    final_response = eceLLM_response if len(eceLLM_response) > len(general_response) else general_response\n",
    "\n",
    "    # Create HTML content with CSS styling for visual presentation\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "        <h2>Query</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Retrieved Context</h2>\n",
    "        <p>{retrieved_context}</p>\n",
    "        <h2>Generated Response</h2>\n",
    "        <p style=\"color: #27ae60;\">{final_response}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example use\n",
    "enhanced_langchain_integration(\"Looking for stylish and comfortable jacket\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef23515",
   "metadata": {},
   "source": [
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "Integrating Langchain with vector RAG requires careful planning and execution, including ensuring data security, optimizing retrieval algorithms, and effectively managing blockchain transactions. The combination of Lanchain's blockchain technology with RAG's NLP capabilities offers a unique opportunity to redefine retail analytics, making it more secure, efficient, and insightful.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The integration of Lanchain with vector RAG represents a significant advancement in retail analytics, offering unprecedented levels of data integrity, retrieval efficiency, and analytical depth. By harnessing these technologies, we can unlock new insights and value from our retail data, driving innovation and excellence in our business operations.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b329373da4c7b86c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
