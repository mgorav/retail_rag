{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a203cf",
   "metadata": {},
   "source": [
    "\n",
    "# Smart Retail Navigator: Unifying RAG, LLM, and Annoy for Advanced Query Intelligence\n",
    "\n",
    "This notebook presents an enhanced analysis using a Structured Retrieval-Augmented Generation (RAG) System, specifically tailored for the retail sector. The system leverages advanced data processing techniques and machine learning models to provide comprehensive insights into retail operations, customer behavior, and sales performance. Through detailed examples and explanations, we aim to demonstrate the application of cutting-edge AI technologies in transforming retail analytics and decision-making processes. The below diagram shows overall architecture:\n",
    "\n",
    "![Architecture](architecture.png)\n",
    "\n",
    "The key aspects of the Smart Retail Navigator system architecture:\n",
    "\n",
    "- **Data Layer**: Manages storage and access of retail data \n",
    "\n",
    "- **Retrieval-Augmented Generation (RAG)**: Retrieves relevant data and generates query responses by combining information retrieval and deep learning\n",
    "\n",
    "- **Large Language Models (LLM)**: Understand queries and generate human-like responses after specialized fine-tuning \n",
    "\n",
    "- **Annoy**: Rapidly retrieves most relevant information for queries via similarity searches in vector spaces\n",
    "\n",
    "- **Query Processor**: Coordinates overall workflow - query understanding by LLM, data retrieval via Annoy and RAG, and response generation\n",
    "\n",
    "- **Analytics Module**: Transforms system outputs into business insights for data-driven decision making\n",
    "\n",
    "The architecture strategically integrates the latest innovations in AI to ensure scalability, efficiency, accuracy and cutting-edge capabilities for enabling advanced retail analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e621bb9",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation and Exploration\n",
    "\n",
    "In this section, we delve into the initial steps of our analysis: preparing and exploring the dataset. Our focus is on understanding the characteristics of the data, identifying patterns, and preparing it for further analysis. We'll cover data loading, cleaning, and basic exploratory data analysis (EDA) techniques that are crucial for any data science project.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b13876",
   "metadata": {},
   "source": [
    "\n",
    "## Predictive Modeling and Analysis\n",
    "\n",
    "Following data preparation, we transition to the core of our analysisâ€”predictive modeling. This section explores the creation and evaluation of models that predict future retail trends based on historical data. We'll discuss model selection, training, and validation, emphasizing the importance of accuracy and reliability in predictions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c87bd",
   "metadata": {},
   "source": [
    "\n",
    "## Insights and Conclusion\n",
    "\n",
    "In the final section, we synthesize our findings into actionable insights. Drawing from the data exploration and predictive modeling phases, we outline key takeaways and recommend strategies for retail businesses to optimize their operations, enhance customer satisfaction, and boost sales. This comprehensive analysis demonstrates the transformative potential of AI in retail.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a15f538ea5c4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T21:17:48.835857Z",
     "start_time": "2024-02-19T21:17:48.785267Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install transformers annoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326c255dbc81696",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Retrieval:** The similarity between a query \\(q\\) and a document \\(d\\) is often computed using cosine similarity:\n",
    "  \\[ S(d, q) = \\frac{d \\cdot q}{\\|d\\| \\|q\\|} \\]\n",
    "- **Generation:** The probability of generating a word \\(w_t\\) given the context \\(C\\) and previous words \\(w_{<t}\\) is modeled as:\n",
    "  \\[ P(w_t | w_{<t}, C) = \\text{softmax}(W_h h_t) \\]\n",
    "\n",
    "### Large Language Models (LLM)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Self-Attention:** The self-attention mechanism's computation is defined as:\n",
    "  \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
    "\n",
    "### LangChain\n",
    "\n",
    "**Core Concepts:**\n",
    "LangChain, developed as an open-source toolkit, simplifies the creation of applications leveraging LLMs by facilitating integration with external computation and data sources. Its components include:\n",
    "\n",
    "- **Schema:** Defines core data structures like Text, ChatMessages, Examples, and Document, essential for interacting with language models.\n",
    "- **Models:** Categorizes into Language Models, Chat Models, and Text Embedding Models, offering interfaces for seamless integration with LLMs.\n",
    "- **Prompts:** Crafting prompts is vital for directing LLMs. LangChain introduces PromptTemplates for constructing prompts dynamically.\n",
    "- **Indexes:** Serve as bridges between documents/data and LLMs, crucial for enriching models with context-specific information.\n",
    "- **Memory:** Facilitates storing and retrieving chat history or conversational context, enhancing the model's ability to produce coherent and context-aware responses.\n",
    "- **Chains:** Enable the sequencing of multiple components, such as data retrieval, prompt generation, and response parsing, into a cohesive workflow.\n",
    "- **Agents:** Utilize LLMs for selecting and sequencing actions, embodying the decision-making prowess of LLMs in application scenarios.\n",
    "\n",
    "**Innovations in LangChain:**\n",
    "LangChain's modular abstractions for these components significantly lower the barrier to integrating complex LLM functionalities into applications. By abstracting the complexities involved in handling language models, data retrieval, and processing, LangChain empowers developers to build more sophisticated, context-aware applications with ease.\n",
    "\n",
    "**Mathematical Extensions:**\n",
    "While LangChain primarily focuses on the architectural and integration aspects of using LLMs, the underlying mathematical principles of its components (especially Models and Indexes) are grounded in the computations of neural networks, vector space models, and embeddings. For instance, the Text Embedding Models convert textual data into numerical vectors, capturing semantic meanings in a high-dimensional space, which can be mathematically represented as:\n",
    "\\[ \\text{Embedding}(text) = V \\]\n",
    "where \\(V\\) is a vector representing the text in a semantic vector space.\n",
    "\n",
    "### Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Tree Construction:** Annoy uses random projection trees for partitioning data, optimizing for both speed and memory efficiency.\n",
    "- **Approximate Search:** The search in Annoy is approximated to quickly retrieve near neighbors without exhaustive search, significantly reducing query time.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "This comprehensive overview, enriched with details from the LangChain components guide and mathematical principles, offers a deeper understanding of the technologies and methodologies driving today's AI applications. LangChain, by abstracting the complexity of integrating LLMs with external data and computational resources, stands out as a pivotal framework for developers aiming to leverage the power of language models in their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a73509e88c59e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mock Data Generation\n",
    "\n",
    "The provided Python function `generate_retail_mock_data` is designed to create mock retail product descriptions. It's a versatile tool for generating a dataset that could be used in various retail or e-commerce data analysis projects, machine learning models, or simply for testing and demonstration purposes. Here's a detailed breakdown of how this function works and its components:\n",
    "\n",
    "### Function Definition and Parameters\n",
    "- **Function Name:** `generate_retail_mock_data`\n",
    "- **Parameters:**\n",
    "  - `categories`: A list of strings representing product categories. Default categories are 'shoes', 'apparel', and 'jackets'.\n",
    "  - `num_items_per_category`: An integer that defines how many items to generate per category. The default is 500.\n",
    "  - `seed`: An integer used to initialize the random number generator, ensuring reproducibility of the generated descriptions. The default is 42.\n",
    "  - `enhance_description`: A boolean flag to indicate whether to enhance a subset of product descriptions using the GPT-2 model. The default is `False`.\n",
    "  - `enhanced_samples`: An integer indicating how many product descriptions to enhance with additional creative sentences. The default is 100.\n",
    "\n",
    "### Core Functionality\n",
    "1. **Initializing the Random Seed:** Ensures reproducibility by initializing the random number generator with a specified seed.\n",
    "\n",
    "2. **Base Description Generation:** Iterates over each category to generate product descriptions, including a unique identifier and a combination of three randomly selected features (e.g., 'lightweight', 'durable').\n",
    "\n",
    "3. **Optional Description Enhancement:** For a limited number of products (defined by `enhanced_samples`), the function uses the GPT-2 model to generate and append an additional creative sentence, enhancing the base description.\n",
    "\n",
    "4. **Compilation of Descriptions:** All generated (and optionally enhanced) descriptions are compiled into a list, `product_descriptions`, which is then returned by the function.\n",
    "\n",
    "### Output\n",
    "- Returns a list, `product_descriptions`, containing all generated (and optionally enhanced) product descriptions.\n",
    "\n",
    "### Example Output\n",
    "An example of the function's output, showing a variety of product descriptions, some of which may be enhanced with creative sentences generated by the GPT-2 model:\n",
    "```\n",
    "Shoes 1 with features: durable stylish comfortable in category shoes.\n",
    "Apparel 1 with features: lightweight waterproof durable in category apparel. \"This apparel combines fashion with function.\"\n",
    "Jackets 1 with features: comfortable stylish waterproof in category jackets.\n",
    "...\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "This mock data generation function is invaluable for:\n",
    "- **Development and Testing:** Providing test data for e-commerce platforms to evaluate search and recommendation systems.\n",
    "- **Data Analysis Projects:** Offering a basis for demonstrations or educational projects focused on retail data analytics.\n",
    "- **Machine Learning Models:** Supplying initial training or testing data for models when actual product data is not available.\n",
    "\n",
    "### Customization and Extension\n",
    "The function offers flexibility for customization:\n",
    "- Adjusting product categories, features, or the number of items to better fit specific needs.\n",
    "- Toggling the description enhancement feature or changing the number of enhanced descriptions.\n",
    "\n",
    "This function demonstrates an effective approach for generating rich, varied, and reproducible mock data for retail-related applications, adaptable to a wide array of use cases in the retail and e-commerce sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4263dd944f97f76d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T21:19:13.791051Z",
     "start_time": "2024-02-19T21:17:48.844540Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are using amazon s4 and it's with features: lightweight comfortable waterproof in category shoes. This way I won't be taking away the opportunity to make the link and I hope it will turn up in the next post I just want to let you\n",
      "<i>[ 11.079073] usbcore with features: waterproof durable lightweight in category shoes. registered new interface driver usbfs [ 11.079684] usbcore: registered new interface driver hub 1 [ 11.079687] usbcore:\n",
      "Name size price\n",
      "\n",
      "product name\n",
      "\n",
      "d with features: durable lightweight waterproof in category shoes. D,D+D+D+D>\n",
      "\n",
      "G:\n",
      "\n",
      "G<D><D>\n",
      "\n",
      "R:\n",
      "\n",
      "R\n",
      "\n",
      "R\n",
      "You can do it with our handy tool. if you're with features: comfortable lightweight waterproof in category shoes. 1. On the internet, a few people actually are just like you with words and pictures in their mouth saying that. If a sentence is not very important then\n",
      "$ name {name}\n",
      "\n",
      "$ name. with features: stylish lightweight comfortable in category shoes. {\n",
      "\n",
      "$ name. $ {name} = \"\".\n",
      "\n",
      "} $ name will be defined as: name. $ {name } = \"\"\n",
      "\"we're going to build a product page and with features: lightweight durable comfortable in category shoes. You're going to get to sell more products\" and then do that sentence with a lot of different words. It could be using \"I'm going to build a company\n",
      "â€¢ change the name of footwear you purchase with features: comfortable lightweight waterproof in category shoes. â€¢ use the following phrases when writing \"I do not want to work with\" or \"I did not choose\" â€¢ use these words when talking with strangers or to make others feel\n",
      "Use a database of all the unique brands in your app store with features: durable stylish lightweight in category shoes. // Create a brand. // Let's include a few of our own. // Check out our brand website, where they've built their own brand brand\n",
      "Note: if the product is not available in your with features: stylish waterproof lightweight in category shoes. You've added too much text. Try editing it. If the product isn't available: You added too much text. Try editing it. If you get stuck, click\n",
      "- to generate a unique unique shoe name for our with features: durable stylish comfortable in category shoes. Create a unique new shoe design in-house using only our brand specific keywords\n",
      "\n",
      "Create creative footwear design in-house using only our brand specific keywords Start on\n",
      "The company's custom apparel website has an all-new range with features: waterproof durable lightweight in category apparel. an entire month of apparel from apparel brands like Adidas, Nike, Nike, and Brooks, plus more. But for now, that's only a sample of the\n",
      "Use the custom-made clothing and accessories as an with features: waterproof lightweight stylish in category apparel. color text on the left of this page\n",
      "\n",
      "A custom-made item, item, or category\n",
      "\n",
      "A pattern for your clothing and apparel\n",
      "\n",
      "Your own style\n",
      "Http://www.dartmouth_shirt.com/ with features: stylish lightweight durable in category apparel. \"Trying for greatness\n",
      "\n",
      "I believe in everything I have, but I could do nothing to change my body and my mind. I can't\n",
      "Https://shop.facebook.com/shop/evan with features: waterproof comfortable lightweight in category apparel. solo I would like to add that one to my email to make sure you get what you are looking for. -Voltage\n",
      "\n",
      "The idea is\n",
      "- choose from a single color with features: stylish lightweight durable in category apparel. #5 (red) #36 (blue) #50 (yellow) #55 (white) #60 (bright)\n",
      "\n",
      "Use a comma-separated list of words (e\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import random\n",
    "\n",
    "# Initialize the model and tokenizer with GPT-2\n",
    "model_name = \"gpt2\"\n",
    "text_generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "def generate_product_names(category, num_names=5):\n",
    "    \"\"\"\n",
    "    Generates unique product names for a given category using GPT-2.\n",
    "    \"\"\"\n",
    "    generated_names = set()\n",
    "    prompt = f\"Generate unique product names for {category}:\"\n",
    "\n",
    "    # Keep generating until we have the desired number of unique names\n",
    "    while len(generated_names) < num_names:\n",
    "        generated_text = text_generator(prompt, max_length=20, num_return_sequences=1)[0]['generated_text']\n",
    "        # Extract names from the generated text\n",
    "        names = generated_text.replace(prompt, \"\").split(',')\n",
    "        for name in names:\n",
    "            clean_name = name.strip().capitalize()\n",
    "            if clean_name and clean_name not in generated_names:\n",
    "                generated_names.add(clean_name)\n",
    "                if len(generated_names) == num_names:\n",
    "                    break\n",
    "\n",
    "    return list(generated_names)\n",
    "\n",
    "def generate_retail_mock_data(categories=['shoes', 'apparel', 'jackets'], num_items_per_category=500, seed=42, enhance_description=False, enhanced_samples=100):\n",
    "    random.seed(seed)\n",
    "    product_descriptions = []\n",
    "    enhanced_descriptions_count = 0\n",
    "\n",
    "    for category in categories:\n",
    "        # Generate unique product names for the category\n",
    "        product_list = generate_product_names(category, num_names=num_items_per_category)\n",
    "\n",
    "        for i, product_name in enumerate(product_list):\n",
    "            base_description = f\"{product_name} with features: \" + \\\n",
    "                               \" \".join(random.sample(['lightweight', 'durable', 'waterproof', 'stylish', 'comfortable'], 3)) + \\\n",
    "                               f\" in category {category}.\"\n",
    "            if enhance_description and enhanced_descriptions_count < enhanced_samples:\n",
    "                # Generate an additional descriptive sentence for a limited number of products\n",
    "                prompt = f\"Write a creative sentence for {product_name.lower()}:\"\n",
    "                enhanced_description = text_generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "                enhanced_description = enhanced_description.replace(prompt, \"\").strip()\n",
    "                full_description = f\"{base_description} {enhanced_description}\"\n",
    "                enhanced_descriptions_count += 1\n",
    "            else:\n",
    "                full_description = base_description\n",
    "            product_descriptions.append(full_description)\n",
    "    return product_descriptions\n",
    "\n",
    "# Example usage\n",
    "mock_retail_product_descriptions = generate_retail_mock_data(enhance_description=True, num_items_per_category=10)  # Reduced for demonstration\n",
    "print(\"\\n\".join(mock_retail_product_descriptions[:15]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b3ec1860473171",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:23:40.600231Z",
     "start_time": "2024-02-19T23:23:40.587807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy index is built with 30 items.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "vector_length = 100  # Assuming a 100-dimensional vector for each product description\n",
    "annoy_index = AnnoyIndex(vector_length, 'angular')\n",
    "\n",
    "# Mock function to convert descriptions to vectors\n",
    "def description_to_vector(description):\n",
    "    np.random.seed(hash(description) % (2**32 - 1))\n",
    "    return np.random.rand(vector_length)\n",
    "\n",
    "# Adding items to Annoy index\n",
    "for i, description in enumerate(mock_retail_product_descriptions):\n",
    "    vec = description_to_vector(description)\n",
    "    annoy_index.add_item(i, vec)\n",
    "\n",
    "annoy_index.build(10)  # Using 10 trees\n",
    "num_products = len(mock_retail_product_descriptions)\n",
    "print(\"Annoy index is built with\", num_products, \"items.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60101a8acb5c7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
    "\n",
    "Annoy is a C++ library with Python bindings designed to efficiently search for points in space that are close to a given query point, emphasizing speed and minimal memory usage while accepting a trade-off in precision. It's particularly useful for implementing recommendation systems, enhancing search engines, and facilitating various machine learning applications where quick nearest neighbor queries are essential.\n",
    "\n",
    "### Key Features:\n",
    "- **Memory-efficient Indexing:** Annoy creates large, read-only, file-based data structures that are memory-mapped, allowing multiple processes to share the same data without duplicating it in memory.\n",
    "- **Incremental Updates:** It supports adding items to the index incrementally without the need to rebuild the index from scratch, making it suitable for dynamic datasets.\n",
    "- **Persistence:** Annoy indexes can be saved to disk and later reloaded, facilitating persistent storage and retrieval of vector data across sessions.\n",
    "\n",
    "### Usage in This Notebook:\n",
    "In the context of this notebook, Annoy is utilized to store and query embeddings of retail product descriptions. Here's how we integrate Annoy for a practical use-case:\n",
    "\n",
    "1. **Embedding Storage:** We generate embeddings for each product description using a mock function that simulates the conversion of textual descriptions into 100-dimensional vectors. These embeddings are stored in an Annoy index.\n",
    "2. **Index Creation:** An AnnoyIndex instance is created with the specified vector length and metric ('angular' for cosine similarity). Each product's embedding vector is added to the index, which is then built with a specified number of trees to optimize query performance.\n",
    "3. **Querying:** For a given query, its embedding is calculated and used to fetch the nearest neighbors from the Annoy index. This process efficiently identifies the most relevant product descriptions based on the similarity of their embeddings to the query.\n",
    "\n",
    "This demonstration showcases the use of Annoy as a vector database for embedding-based retrieval, illustrating its application in scenarios where finding similar items quickly is crucial, such as in retail product recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29016635cb7d53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Integration of LangChain with RAG and Annoy\n",
    "\n",
    "In this demonstration, we explore the integration of LangChain with Retrieval-Augmented Generation (RAG) and Annoy, showcasing a seamless workflow that combines the retrieval capabilities of Annoy with the generative prowess of a Large Language Model (LLM) provided by Hugging Face's `transformers`. This integration exemplifies how to leverage the strengths of both retrieval and generation to enhance the relevance and richness of generated text based on a given query.\n",
    "\n",
    "### Workflow Overview:\n",
    "1. **Query Embedding:** For a given query, we first convert it into an embedding vector using a mock function `description_to_vector`. This function simulates the process of transforming textual data into a numerical representation that can be processed by machine learning models.\n",
    "\n",
    "2. **Retrieval with Annoy:** Using the query's vector representation, we retrieve the nearest neighbor product descriptions from an Annoy index. Annoy efficiently identifies the vectors in the dataset that are closest to the query vector, based on cosine similarity. This step highlights Annoy's utility in quickly fetching relevant data from a large dataset.\n",
    "\n",
    "3. **Context Assembly:** The retrieved product descriptions are concatenated to form a context string. This assembled context is then used to inform the generation process, ensuring that the generated text is relevant to the specifics of the query.\n",
    "\n",
    "4. **Text Generation with LangChain and RAG:** The concatenated context is fed into a generative model from the `transformers` library, specifically `distilgpt2`, alongside the original query. This generative step uses the context to produce a response that is not only contextually aware but also creatively enriched by the language model.\n",
    "\n",
    "5. **HTML Presentation:** The query, retrieved context, and generated response are presented in an HTML format for enhanced readability. This step demonstrates how the integration of these technologies can be used to create user-friendly outputs for applications such as product recommendation systems or automated customer service responses.\n",
    "\n",
    "### Example Usage:\n",
    "In our example, we query for \"Looking for stylish and comfortable shoes.\" The process involves embedding the query, retrieving related product descriptions using Annoy, and generating a response that synthesizes the query intent with the retrieved information. The final output is displayed in a styled HTML block, making it easy to visualize the integration's effectiveness in producing relevant and engaging content.\n",
    "\n",
    "This integration exemplifies a practical application of combining retrieval and generative models to enhance the capabilities of AI-driven systems. By leveraging the specific strengths of Annoy for efficient data retrieval and the generative capabilities of language models like `distilgpt2`, developers can create sophisticated solutions that address complex queries with contextually rich and relevant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a1d6191e5a7116",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:23:53.570457Z",
     "start_time": "2024-02-19T23:23:51.813721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/gauravmalhotra/Documents/MyGithub/retail_in_rag/rag_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n        <h2>Query</h2>\n        <p>Looking for stylish and comfortable shoes</p>\n        <h2>Retrieved Context</h2>\n        <p>The company's custom apparel website has an all-new range with features: waterproof durable lightweight in category apparel. an entire month of apparel from apparel brands like Adidas, Nike, Nike, and Brooks, plus more. But for now, that's only a sample of the Buy a new with features: stylish comfortable durable in category jackets. A. Learn how you want to say something or get to know someone.\n\nB. Write a creative sentence.\n\nC. Write a simple sentence.\n\nD. Write a Use a database of all the unique brands in your app store with features: durable stylish lightweight in category shoes. // Create a brand. // Let's include a few of our own. // Check out our brand website, where they've built their own brand brand \"we're going to build a product page and with features: lightweight durable comfortable in category shoes. You're going to get to sell more products\" and then do that sentence with a lot of different words. It could be using \"I'm going to build a company Note: if the product is not available in your with features: stylish waterproof lightweight in category shoes. You've added too much text. Try editing it. If the product isn't available: You added too much text. Try editing it. If you get stuck, click</p>\n        <h2>Response</h2>\n        <p style=\"color: #27ae60;\">Query: Looking for stylish and comfortable shoes. Based on: The company's custom apparel website has an all-new range with features: waterproof durable lightweight in category apparel. an entire month of apparel from apparel brands like Adidas, Nike, Nike, and Brooks, plus more. But for now, that's only a sample of the Buy a new with features: stylish comfortable durable in category jackets. A. Learn how you want to say something or get to know someone.\n\nB. Write a creative sentence.\n\nC. Write a simple sentence.\n\nD. Write a Use a database of all the unique brands in your app store with features: durable stylish lightweight in category shoes. // Create a brand. // Let's include a few of our own. // Check out our brand website, where they've built their own brand brand \"we're going to build a product page and with features: lightweight durable comfortable in category shoes. You're going to get to sell more products\" and then do that sentence with a lot of different words. It could be using \"I'm going to build a company Note: if the product is not available in your with features: stylish waterproof lightweight in category shoes. You've added too much text. Try editing it. If the product isn't available: You added too much text. Try editing it. If you get stuck, click Comment</p>\n    </div>\n    "
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import HTML\n",
    "\n",
    "def simple_langchain_integration_v2(query):\n",
    "    generator = pipeline('text-generation', model='distilgpt2')\n",
    "    # Assuming description_to_vector and other necessary parts are defined elsewhere\n",
    "    query_vector = description_to_vector(query)\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(query_vector, 5)  # Assuming annoy_index is defined elsewhere\n",
    "    retrieved_docs = [mock_retail_product_descriptions[i] for i in nearest_ids]  # Assuming mock_retail_product_descriptions is defined elsewhere\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "    # Added truncation=True to avoid the warning\n",
    "    response = generator(f'Query: {query}. Based on: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)\n",
    "    generated_text = response[0]['generated_text']\n",
    "\n",
    "    # Create HTML content with CSS styling for better visual presentation\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "        <h2>Query</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Retrieved Context</h2>\n",
    "        <p>{retrieved_context}</p>\n",
    "        <h2>Response</h2>\n",
    "        <p style=\"color: #27ae60;\">{generated_text}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example use\n",
    "simple_langchain_integration_v2(\"Looking for stylish and comfortable shoes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266d5d8",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive demonstration of integrating Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) from Hugging Face, leveraging Annoy for efficient nearest neighbor searches, all facilitated through the LangChain framework. The primary focus was on a retail context, showcasing how the amalgamation of retrieval systems and generative models can significantly enhance application capabilities in delivering relevant and contextually rich responses to user queries. Here's a summary of the workflow and key takeaways:\n",
    "\n",
    "- **What:** The notebook outlines the process of generating mock retail product descriptions, indexing these descriptions using Annoy for fast retrieval, and then integrating this with a generative model from Hugging Face (distilGPT-2) for text generation, all orchestrated using the LangChain framework.\n",
    "\n",
    "- **Why:** The integration aims to demonstrate the power of combining vector space retrieval with generative AI to improve the relevance and specificity of responses in a simulated retail query scenario. This approach exemplifies how businesses can leverage AI to enhance user experiences through personalized and context-aware interactions.\n",
    "\n",
    "- **How:**\n",
    "  - **Mock Data Generation:** We started by generating a dataset of mock product descriptions to simulate a retail inventory.\n",
    "  - **Annoy Indexing:** The descriptions were then converted into vector embeddings and indexed using Annoy, enabling efficient similarity-based retrieval.\n",
    "  - **LangChain Integration:** We showcased how LangChain can facilitate the integration of Annoy with a generative LLM to process user queries, retrieve contextually relevant product descriptions, and generate informative responses.\n",
    "  - **Text Generation and Display:** Utilizing the Hugging Face `pipeline` for text generation, the notebook demonstrated generating responses based on the context provided by Annoy's nearest neighbor search, with the output presented in an HTML format for enhanced readability.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Efficiency and Relevance:** The use of Annoy for embedding storage and retrieval showcases an efficient method to add contextual relevance to LLM-generated responses.\n",
    "- **Scalability:** This approach illustrates how scalable solutions can be built for real-world applications, accommodating large datasets typical in retail environments.\n",
    "- **Customizability:** The modular nature of LangChain, combined with the flexibility of Annoy and the generative power of LLMs, underscores the potential for customization according to specific application needs or domains.\n",
    "- **User Experience Enhancement:** The integration exemplifies how AI can be used to significantly enhance user experience, providing a foundation for developing advanced AI-driven retail applications.\n",
    "\n",
    "Through this integration, we've demonstrated a scalable and efficient method to bring context-awareness and relevance to AI-generated text, paving the way for innovative applications in retail and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0d60b19d9e54d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Detailed System Architecture Overview\n",
    "\n",
    "This section delves into the architecture of the system implemented in this notebook, leveraging the Mermaid syntax for a comprehensive visual depiction. We intricately integrate Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) from Hugging Face, employing the Annoy library for efficient similarity searches, all orchestrated within the LangChain framework to enhance retail query processing.\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "![RAG Sequence Diagram](rag_sequence_digram.png)\n",
    "\n",
    "### Architecture Components and Interactions\n",
    "\n",
    "- **User Query:** Initiates the process, where users enter their queries regarding retail products, starting the interaction flow.\n",
    "\n",
    "- **LangChain Framework:** Serves as the central orchestrator, efficiently managing the workflow from user query input to fetching nearest neighbors via Annoy and generating responses through LLM. It ensures seamless integration and communication between different components.\n",
    "\n",
    "- **Annoy Index:** A critical component that stores pre-processed vector embeddings of product descriptions, enabling quick and efficient retrieval of items similar to the user query.\n",
    "\n",
    "- **Retrieve Nearest Neighbors:** This step is crucial for identifying contextually relevant product descriptions based on the user's query, which are then utilized to inform the response generation process.\n",
    "\n",
    "- **LLM (Hugging Face):** At this stage, the system leverages a pre-trained language model to generate detailed and contextually enriched responses by incorporating both the original query and the context provided by the nearest neighbors.\n",
    "\n",
    "- **Generate Response:** The synthesis of input and retrieved context through the LLM culminates in the generation of a response tailored to the user's query, embodying the integration of RAG principles.\n",
    "\n",
    "- **Display Results:** The final step where the system presents the generated response back to the user, completing the cycle and providing a cohesive answer to the query.\n",
    "\n",
    "### Highlighted Features:\n",
    "\n",
    "- **Rapid Context Retrieval:** Utilizes Annoy for the swift retrieval of relevant context, significantly speeding up the response generation process.\n",
    "\n",
    "- **Scalable System Design:** Engineered to accommodate the extensive and growing datasets typical in retail, ensuring the system's scalability.\n",
    "\n",
    "- **Modular and Flexible:** The architecture's modular design promotes flexibility, allowing for the easy replacement or enhancement of individual components, such as swapping the LLM model or modifying the retrieval strategy.\n",
    "\n",
    "- **Enhanced User Engagement:** By delivering precise, context-aware responses, the system significantly improves user interaction and satisfaction, showcasing the potential of AI in transforming retail experiences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbf53842bc35dc0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:23:53.575636Z",
     "start_time": "2024-02-19T23:23:53.571440Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf7700d",
   "metadata": {},
   "source": [
    "# Enhancing E-commerce with NingLab eCeLLM-S\n",
    "\n",
    "The integration of NingLab's eCeLLM-S model into our retail analytics framework signifies a leap forward in applying advanced Large Language Models (LLMs) to the e-commerce domain. This section delves into the technical underpinnings, objectives, and implementation strategy of leveraging eCeLLM-S for enriching e-commerce content and customer interaction.\n",
    "\n",
    "## Background on LLMs and eCeLLM-S\n",
    "\n",
    "Large Language Models (LLMs) like GPT-3 have revolutionized the field of natural language processing (NLP) with their ability to generate coherent and contextually relevant text across a broad spectrum of applications. Building on this foundation, eCeLLM-S is designed to specifically address the nuances and complexities of e-commerce text generation, from product descriptions to customer engagement.\n",
    "\n",
    "### Core Technologies\n",
    "\n",
    "eCeLLM-S employs a transformer-based architecture, renowned for its self-attention mechanism, which allows the model to weigh the importance of different parts of the input text differently. This is crucial for generating text that is not only grammatically correct but also contextually aligned with the subject matter.\n",
    "\n",
    "#### Mathematical Foundation of Transformers\n",
    "\n",
    "The transformer model utilizes several key equations, including the self-attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "- $Q$, $K$, and $V$ represent the query, key, and value vectors, respectively.\n",
    "- $d_k$ is the dimensionality of the key vectors, providing a normalization factor.\n",
    "\n",
    "This formula allows the model to dynamically focus on different parts of the input sequence, enhancing the relevance and coherence of the generated text.\n",
    "\n",
    "### Objectives of eCeLLM-S Integration\n",
    "\n",
    "- **Contextual Relevance**: Generate text that reflects the specific context of retail products and customer interactions.\n",
    "- **Enhanced Descriptions**: Create detailed and engaging product descriptions that capture the essence and appeal of products.\n",
    "- **Customer Engagement**: Facilitate improved customer interaction through personalized and informative responses.\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "The effective deployment of eCeLLM-S within our retail analysis ecosystem involves a multi-faceted approach:\n",
    "\n",
    "1. **Data Curation**: Assemble a diverse dataset that encompasses a wide range of our retail products and customer feedback.\n",
    "2. **Model Fine-Tuning**: Adapt eCeLLM-S to our specific retail context to enhance its output's relevance and accuracy.\n",
    "3. **Workflow Integration**: Seamlessly incorporate eCeLLM-S-generated content into our product listings and customer service processes.\n",
    "4. **Iterative Refinement**: Continuously monitor and refine the model's performance based on user feedback and emerging retail trends.\n",
    "\n",
    "\n",
    "**The strategic integration of NingLab's eCeLLM-S model represents a forward-thinking approach to harnessing the power of LLMs in the retail sector. By leveraging its advanced text generation capabilities, we can significantly enhance the quality of our product descriptions and the effectiveness of our customer interactions, setting a new benchmark for AI-driven e-commerce excellence.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddfc93",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-19T23:23:53.576405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2501c28fadf74f5ba2c05e8529ef2574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the eCeLLM-S pipeline for text generation\n",
    "eCeLLM_S_pipeline = pipeline(\"text-generation\", model=\"NingLab/eCeLLM-S\")\n",
    "\n",
    "# Example usage with a prompt related to retail\n",
    "prompt = \"Generate a catchy product description for a pair of eco-friendly sneakers\"\n",
    "generated_text = eCeLLM_S_pipeline(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated product description:\")\n",
    "print(generated_text[0]['generated_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33273fd9",
   "metadata": {},
   "source": [
    "\n",
    "### Application in Retail Analysis\n",
    "\n",
    "The above example demonstrates the use of eCeLLM-S in generating a product description. Such capabilities can be extended to various aspects of retail analysis, including but not limited to, generating creative product names, detailed product descriptions, and personalized marketing messages.\n",
    "\n",
    "By integrating eCeLLM-S, we can enhance the customer experience through more engaging and informative content, contributing to improved customer satisfaction and sales.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90eee7",
   "metadata": {},
   "source": [
    "\n",
    "## Integration with Lanchain Using Vector RAG\n",
    "\n",
    "To further enhance our retail analytics capabilities, we integrate Lanchain with vector Retrieval-Augmented Generation (RAG) for advanced query understanding and information retrieval. This integration aims to leverage the synergies between Lanchain's blockchain technology and RAG's powerful retrieval capabilities to enhance data veracity and retrieval efficiency in our retail analysis.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- **Enhance Data Veracity**: Utilize Lanchain to ensure the integrity and veracity of the retail data used in our analysis.\n",
    "- **Improve Retrieval Efficiency**: Leverage vector RAG for efficient retrieval of relevant information from our extensive retail dataset.\n",
    "- **Innovate Retail Analytics**: Combine blockchain technology and state-of-the-art NLP to pioneer innovative retail analytics solutions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38f316",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Function to integrate language model with retail data\n",
    "def enhanced_langchain_integration(query):\n",
    "    # Using eCeLLM-S for e-commerce specific generation and DistilGPT-2 for general text\n",
    "    eceLLM_generator = pipeline('text-generation', model='NingLab/eCeLLM-S')  # Adjust the model ID as needed\n",
    "    general_generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "    # Convert query to vector (Assuming this function is defined and works with your data)\n",
    "    query_vector = description_to_vector(query)\n",
    "\n",
    "    # Retrieve similar product descriptions based on the query vector\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(query_vector, 5)  # Assuming annoy_index is properly set up\n",
    "    retrieved_docs = [mock_retail_product_descriptions[i] for i in nearest_ids]  # Assuming this variable holds your mock data\n",
    "\n",
    "    # Combine retrieved context for better generation\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "\n",
    "    # Generate response with both models and choose based on context\n",
    "    eceLLM_response = eceLLM_generator(f'Query: {query}. Based on: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
    "    general_response = general_generator(f'Query: {query}. Context: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
    "\n",
    "    # Choose response based on some criteria (e.g., length, relevance, etc.) - for simplicity, just using eCeLLM-S here\n",
    "    final_response = eceLLM_response if len(eceLLM_response) > len(general_response) else general_response\n",
    "\n",
    "    # Create HTML content with CSS styling for visual presentation\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "        <h2>Query</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Retrieved Context</h2>\n",
    "        <p>{retrieved_context}</p>\n",
    "        <h2>Generated Response</h2>\n",
    "        <p style=\"color: #27ae60;\">{final_response}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example use\n",
    "enhanced_langchain_integration(\"Looking for stylish and comfortable shoe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "Integrating Langchain with vector RAG requires careful planning and execution, including ensuring data security, optimizing retrieval algorithms, and effectively managing blockchain transactions. The combination of Lanchain's blockchain technology with RAG's NLP capabilities offers a unique opportunity to redefine retail analytics, making it more secure, efficient, and insightful.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The integration of Lanchain with vector RAG represents a significant advancement in retail analytics, offering unprecedented levels of data integrity, retrieval efficiency, and analytical depth. By harnessing these technologies, we can unlock new insights and value from our retail data, driving innovation and excellence in our business operations.\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e47d7387aeb21df4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "#### **Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures the frequency of a word in a document. TF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in a document } d}{\\text{Total number of terms in the document } d}\n",
    "$$\n",
    "\n",
    "#### **Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is within the entire corpus. Words that appear frequently in one document but less frequently in the corpus receive a higher weighting. IDF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } D}{\\text{Number of documents with term } t \\text{ in it}} + 1 \\right)\n",
    "$$\n",
    "\n",
    "Note: Adding 1 in the denominator prevents division by zero for terms that appear in all documents.\n",
    "\n",
    "#### **TF-IDF Calculation**\n",
    "\n",
    "The TF-IDF value is calculated by multiplying TF and IDF:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "This results in a matrix where each row represents a document and each column represents a term in the corpus, with values indicating the significance of each term to the document.\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a machine learning algorithm for dimensionality reduction well-suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "#### **Mathematical Foundation**\n",
    "\n",
    "Given a set of points in a high-dimensional space, t-SNE first computes probabilities \\(p_{ij}\\) that are proportional to the similarity of objects \\(x_i\\) and \\(x_j\\), as follows:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
    "$$\n",
    "\n",
    "In the low-dimensional space, t-SNE computes similar probabilities \\(q_{ij}\\) using a Student-t distribution:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "The Kullback-Leibler divergence between the distributions \\(P\\) and \\(Q\\) is minimized by gradient descent:\n",
    "\n",
    "$$\n",
    "C = \\text{KL}(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "#### **Perplexity**\n",
    "\n",
    "Perplexity is a parameter for t-SNE that suggests how to balance attention between local and global aspects of your data and is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = 2^{H(P_i)}\n",
    "$$\n",
    "\n",
    "where \\(H(P_i)\\) is the Shannon entropy of \\(P_i\\) measured in bits.\n",
    "\n",
    "### Application in the Notebook Context\n",
    "\n",
    "In the context of the notebook, TF-IDF is used to transform mock retail product descriptions into numerical vectors, capturing the importance of terms within descriptions relative to their frequency across all descriptions. These vectors serve as features that represent each product's textual information.\n",
    "\n",
    "t-SNE is then applied to these TF-IDF vectors to reduce the high-dimensional feature space to a 2D space suitable for visualization. The `perplexity` parameter is dynamically adjusted based on the dataset size to ensure meaningful dimensionality reduction.\n",
    "\n",
    "The final visualization with seaborn plots the t-SNE-transformed points, colored by product categories, to illustrate the distribution and clustering of products based on their descriptions. This process highlights the power of combining TF-IDF for feature extraction with t-SNE for visualization, providing insights into the semantic relationships within the dataset.\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "#### **Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures the frequency of a word in a document. TF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in a document } d}{\\text{Total number of terms in the document } d}\n",
    "$$\n",
    "\n",
    "#### **Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is within the entire corpus. Words that appear frequently in one document but less frequently in the corpus receive a higher weighting. IDF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } D}{\\text{Number of documents with term } t \\text{ in it}} + 1 \\right)\n",
    "$$\n",
    "\n",
    "Note: Adding 1 in the denominator prevents division by zero for terms that appear in all documents.\n",
    "\n",
    "#### **TF-IDF Calculation**\n",
    "\n",
    "The TF-IDF value is calculated by multiplying TF and IDF:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "This results in a matrix where each row represents a document and each column represents a term in the corpus, with values indicating the significance of each term to the document.\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a machine learning algorithm for dimensionality reduction well-suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "#### **Mathematical Foundation**\n",
    "\n",
    "Given a set of points in a high-dimensional space, t-SNE first computes probabilities \\(p_{ij}\\) that are proportional to the similarity of objects \\(x_i\\) and \\(x_j\\), as follows:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
    "$$\n",
    "\n",
    "In the low-dimensional space, t-SNE computes similar probabilities \\(q_{ij}\\) using a Student-t distribution:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "The Kullback-Leibler divergence between the distributions \\(P\\) and \\(Q\\) is minimized by gradient descent:\n",
    "\n",
    "$$\n",
    "C = \\text{KL}(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "#### **Perplexity**\n",
    "\n",
    "Perplexity is a parameter for t-SNE that suggests how to balance attention between local and global aspects of your data and is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = 2^{H(P_i)}\n",
    "$$\n",
    "\n",
    "where \\(H(P_i)\\) is the Shannon entropy of \\(P_i\\) measured in bits.\n",
    "\n",
    "### Application in the Notebook Context\n",
    "\n",
    "In the context of the notebook, TF-IDF is used to transform mock retail product descriptions into numerical vectors, capturing the importance of terms within descriptions relative to their frequency across all descriptions. These vectors serve as features that represent each product's textual information.\n",
    "\n",
    "t-SNE is then applied to these TF-IDF vectors to reduce the high-dimensional feature space to a 2D space suitable for visualization. The `perplexity` parameter is dynamically adjusted based on the dataset size to ensure meaningful dimensionality reduction.\n",
    "\n",
    "The final visualization with seaborn plots the t-SNE-transformed points, colored by product categories, to illustrate the distribution and clustering of products based on their descriptions. This process highlights the power of combining TF-IDF for feature extraction with t-SNE for visualization, providing insights into the semantic relationships within the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28666c8fb07d81b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Combine the descriptions into a single string per product for TF-IDF\n",
    "descriptions = [desc.split(\" with features: \")[0] for desc in mock_retail_product_descriptions]\n",
    "\n",
    "# Use TF-IDF to simulate embeddings\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_embeddings = vectorizer.fit_transform(descriptions).toarray()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b329373da4c7b86c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Adjust perplexity to be less than the number of samples\n",
    "# Assuming 'tfidf_embeddings' contains your dataset's embeddings\n",
    "n_samples = tfidf_embeddings.shape[0]\n",
    "perplexity_value = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "tsne_results = tsne.fit_transform(tfidf_embeddings)\n",
    "\n",
    "tsne_results\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "41bb8e862fade2cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Function to compute pairwise distances\n",
    "def compute_pairwise_distances(X):\n",
    "    distances = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(i+1, X.shape[0]):\n",
    "            distances[i, j] = np.linalg.norm(X[i] - X[j])\n",
    "            distances[j, i] = distances[i, j]  # Distance matrix is symmetric\n",
    "    return distances\n",
    "\n",
    "# Function to compute similarity matrix\n",
    "def compute_similarity_matrix(distances, sigma):\n",
    "    similarities = np.exp(-distances**2 / (2 * sigma**2))\n",
    "    np.fill_diagonal(similarities, 0)  # Diagonal elements are set to 0\n",
    "    return similarities\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(P):\n",
    "    epsilon = 1e-12  # Small epsilon value to avoid numerical issues\n",
    "    P = np.maximum(P, epsilon)  # Replace zero values with epsilon\n",
    "    entropy = -np.sum(P * np.log2(P))\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n",
    "\n",
    "# Combine the descriptions into a single string per product for TF-IDF\n",
    "descriptions = [desc.split(\" with features: \")[0] for desc in mock_retail_product_descriptions]\n",
    "\n",
    "# Use TF-IDF to simulate embeddings\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_embeddings = vectorizer.fit_transform(descriptions).toarray()\n",
    "\n",
    "# Adjust perplexity to be less than the number of samples\n",
    "n_samples = tfidf_embeddings.shape[0]\n",
    "perplexity_value = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "\n",
    "# Perform t-SNE embedding\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "tsne_results = tsne.fit_transform(tfidf_embeddings)\n",
    "\n",
    "# Compute pairwise distances\n",
    "pairwise_distances = compute_pairwise_distances(tsne_results)\n",
    "\n",
    "# Compute similarity matrix\n",
    "sigma = 1.0  # Adjust sigma according to your data\n",
    "similarity_matrix = compute_similarity_matrix(pairwise_distances, sigma)\n",
    "\n",
    "# Compute perplexity\n",
    "perplexity = compute_perplexity(similarity_matrix)\n",
    "\n",
    "# Create a DataFrame to summarize the computed values and provide analysis\n",
    "data = {\n",
    "    \"Metric\": [\"Pairwise Distances\", \"Similarity Matrix\", \"Perplexity\"],\n",
    "    \"Value\": [pairwise_distances, similarity_matrix, perplexity],\n",
    "    \"Analysis\": [\n",
    "        \"Pairwise distances between points in the t-SNE embedding space. High values indicate more dissimilar points.\",\n",
    "        \"Measure of similarity between data points in the embedding space. High values indicate high similarity.\",\n",
    "        \"A measure of how well the probability distribution predicts a sample. Higher values indicate a smoother embedding.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set option to display complete DataFrame content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the DataFrame\n",
    "display(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "784dc1fb79319453",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to compute pairwise distances\n",
    "def compute_pairwise_distances(X):\n",
    "    distances = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(i+1, X.shape[0]):\n",
    "            distances[i, j] = np.linalg.norm(X[i] - X[j])\n",
    "            distances[j, i] = distances[i, j]  # Distance matrix is symmetric\n",
    "    return distances\n",
    "\n",
    "# Function to compute similarity matrix\n",
    "def compute_similarity_matrix(distances, sigma):\n",
    "    similarities = np.exp(-distances**2 / (2 * sigma**2))\n",
    "    np.fill_diagonal(similarities, 0)  # Diagonal elements are set to 0\n",
    "    return similarities\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(P):\n",
    "    epsilon = 1e-12  # Small epsilon value to avoid numerical issues\n",
    "    P = np.maximum(P, epsilon)  # Replace zero values with epsilon\n",
    "    entropy = -np.sum(P * np.log2(P))\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n",
    "\n",
    "# Combine the descriptions into a single string per product for TF-IDF\n",
    "descriptions = [desc.split(\" with features: \")[0] for desc in mock_retail_product_descriptions]\n",
    "\n",
    "# Use TF-IDF to simulate embeddings\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_embeddings = vectorizer.fit_transform(descriptions).toarray()\n",
    "\n",
    "# Adjust perplexity to be less than the number of samples\n",
    "n_samples = tfidf_embeddings.shape[0]\n",
    "perplexity_value = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "\n",
    "# Perform t-SNE embedding\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "tsne_results = tsne.fit_transform(tfidf_embeddings)\n",
    "\n",
    "# Compute pairwise distances\n",
    "pairwise_distances = compute_pairwise_distances(tsne_results)\n",
    "\n",
    "# Compute similarity matrix\n",
    "sigma = 1.0  # Adjust sigma according to your data\n",
    "similarity_matrix = compute_similarity_matrix(pairwise_distances, sigma)\n",
    "\n",
    "# Compute perplexity\n",
    "perplexity = compute_perplexity(similarity_matrix)\n",
    "print(\"Perplexity:\", perplexity)\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=tsne_results[:,0], y=tsne_results[:,1], hue=categories, palette=\"viridis\")\n",
    "plt.title('t-SNE Visualization of Mock Product Descriptions')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(title='Category')\n",
    "plt.show()\n",
    "\n",
    "# Prescriptive Analysis\n",
    "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "prescriptive_analysis = \"Based on the t-SNE visualization, we can observe that products in the 'apparel' category tend to cluster together in the lower left region of the plot. This suggests that these products share similar characteristics or features. To capitalize on this insight, the retail company could create targeted marketing campaigns or promotions specifically tailored to customers interested in apparel products. Additionally, the company could consider expanding its apparel product line or introducing new apparel-related offerings to meet the demand demonstrated by the clustering pattern.\"\n",
    "recommendation = generator(prescriptive_analysis, max_length=100, do_sample=False)[0]['generated_text']\n",
    "print(\"Prescriptive Analysis Recommendation:\", recommendation)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aeefac8fb9e9d4ce",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Research Paper - Enhancing Retail Search Experience through Collaborative Agents and Retrieval-Augmented Models\n",
    "\n",
    "### Abstract\n",
    "This paper presents a groundbreaking approach to retail search by combining interactive shop/retail search agents and advanced query intelligence. By leveraging Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Approximate Nearest Neighbors (Annoy), we propose a novel framework. This framework, incorporating the ShopSearch Framework and the Smart Retail Navigator, offers a sophisticated solution for enhancing search experiences in dynamic retail settings.\n",
    "\n",
    "### Introduction\n",
    "- Highlighting the necessity for improved search mechanisms within the retail sector.\n",
    "- Discussion on the potential of interactive shop/retail search agents to elevate search experiences.\n",
    "- Presentation of the Smart Retail Navigator, integrating the ShopSearch Framework with RAG, LLM, and Annoy technologies.\n",
    "\n",
    "### Related Work\n",
    "- Examination of scholarly works on LLMs in search contexts, interactive shop/retail search agents, RAG in query processing, and Annoy for data retrieval.\n",
    "- Identification of technological shortcomings and the rationale for the proposed framework.\n",
    "\n",
    "### Methodology\n",
    "- Elaboration on the ShopSearch Framework, detailing its interactive shop/retail search features and integration with communication platforms like Slack.\n",
    "- Examination of the Smart Retail Navigator, emphasizing:\n",
    "    - The utilization of RAG and LLMs for sophisticated query handling and response formulation.\n",
    "    - The application of Annoy for rapid similarity searches in extensive product datasets.\n",
    "\n",
    "### Mathematical Models and Equations\n",
    "\n",
    "#### Query Understanding and Reformulation\n",
    "- **Equation for query vector representation**:\n",
    "\n",
    "\\begin{document}\n",
    "\\begin{align*}\n",
    "Q_v &= \\sum_{i=1}^{n} w_i \\cdot \\text{embed}(q_i) \\\\\n",
    "\\intertext{where}\n",
    "Q_v &\\text{ represents the query vector,} \\\\\n",
    "w_i &\\text{ is the weight of term } i, \\\\\n",
    "q_i &\\text{ is the term, and} \\\\\n",
    "\\text{embed} &\\text{ signifies the embedding function.}\n",
    "\\end{align*}\n",
    "\\end{document}\n",
    "\n",
    "- **Reformulation mechanism**:\n",
    "\\begin{document}\n",
    "\\begin{align*}\n",
    "Q_r &= \\text{RAG}(Q_v, C) \\\\\n",
    "\\intertext{where}\n",
    "Q_r &\\text{ denotes the reformulated query,} \\\\\n",
    "\\text{RAG} &\\text{ symbolizes the retrieval-augmented generation process, and} \\\\\n",
    "C &\\text{ indicates the context.}\n",
    "\\end{align*}\n",
    "\\end{document}\n",
    "\n",
    "#### Search Result Ranking\n",
    "- **Relevance scoring equation**:\n",
    "\\begin{document}\n",
    "\\begin{align*}\n",
    "S_r &= \\alpha \\cdot \\text{cos}(Q_r, D_v) + \\beta \\cdot \\text{Popularity}(D) \\\\\n",
    "\\intertext{where}\n",
    "S_r &\\text{ is the relevance score for a document } D, \\\\\n",
    "D_v &\\text{ its vector representation,} \\\\\n",
    "\\alpha, \\beta &\\text{ are adjustable parameters, and} \\\\\n",
    "\\text{cos} &\\text{ denotes the cosine similarity between vectors.}\n",
    "\\end{align*}\n",
    "\\end{document}\n",
    "\n",
    "\n",
    "#### Annoy Index for Efficient Data Retrieval\n",
    "- **Approximate nearest neighbors equation**:\n",
    "\\begin{document}\n",
    "\\begin{align*}\n",
    "N_n &= \\text{Annoy}(D_v, k) \\\\\n",
    "\\intertext{where}\n",
    "N_n &\\text{ outlines the } k \\text{ nearest neighbors to } D_v \\text{ in a high-dimensional space, as determined by Annoy.}\n",
    "\\end{align*}\n",
    "\\end{document}\n",
    "\n",
    "### Implementation\n",
    "- Detailing the architectural specifics, including interactions among the Slack interface, ShopSearch Framework, and Smart Retail Navigator components.\n",
    "- Providing code excerpts and explanations for pivotal functionalities like query reformulation and search result amalgamation.\n",
    "\n",
    "### Case Study: Smart Retail Navigator\n",
    "- A real-world implementation scenario within retail, showcasing system interaction for product and information discovery.\n",
    "- Evaluation of system efficacy through metrics such as search precision, response timeliness, and user satisfaction.\n",
    "\n",
    "### Discussion\n",
    "- Reflecting on the implications of fusing interactive shop/retail search agents with advanced query intelligence for retail.\n",
    "- Addressing encountered challenges and proposing potential resolutions.\n",
    "\n",
    "### Conclusion\n",
    "- Recapitulating the study's findings and its contributions to the domains of retail search and interactive shop/retail search technologies.\n",
    "- Suggesting avenues for further research and enhancements to the Smart Retail Navigator.\n",
    "\n",
    "### References\n",
    "- Listing all referenced works in a standard citation format.\n",
    "\n",
    "### Design\n",
    "\n",
    "1. **System Architecture**: This diagram would showcase the entire system's architecture, highlighting the interconnections between the Slack interface, the ShopSearch Framework, the RAG module, and the Anoy index.\n",
    "![System Architecture](system_architecture.mermaid)\n",
    "\n",
    "2. **Query Processing Workflow**: This diagram would detail the sequence of steps from receiving a query to generating a response, highlighting the roles of RAG and Annoy in the process.\n",
    "\n",
    "![Query Processing Workflow](query_processing_workflow.png)\n",
    "\n",
    "3. **Data Flow Diagram**: Illustrating the flow of data through the system, from initial user query to the final response.\n",
    "\n",
    "![Data Flow](data_flow.png)\n",
    "\n",
    "4. **Annoy Index Search Process**: Visualizing the inner workings of the Annoy index during a search operation.\n",
    "\n",
    "![Annoy Index Search Process](annoy_query_processing_flow.png)\n",
    "\n",
    "By restructuring the content with the inclusion of mathematical equations and Mermaid diagram descriptions, we ensure a comprehensive and visually engaging presentation of the research findings, methodologies, and the theoretical underpinnings of the Smart Retail Navigator system.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., & Burtsev, M. (2020). ConvAI3: Generating clarifying questions for open-domain dialogue systems (ClariQ). arXiv preprint arXiv:2009.11352.\n",
    "2. Amershi, S., & Morris, M. R. (2008). CoSearch: a system for co-located collaborative web search. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 1647â€“1656).\n",
    "3. Avula, S., & Arguello, J. (2020). Wizard of oz interface to study system initiative for conversational search. In Proceedings of the 2020 conference on human information interaction and retrieval (pp. 447â€“451).\n",
    "4. Avula, S., Arguello, J., Capra, R., Dodson, J., Huang, Y., & Radlinski, F. (2019). Embedding search into a conversational platform to support interactive shop/retail search. In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval (pp. 15â€“23).\n",
    "5. Avula, S., Chadwick, G., Arguello, J., & Capra, R. (2018). Searchbots: User engagement with chatbots during interactive shop/retail search. In Proceedings of the 2018 conference on human information interaction & retrieval (pp. 52â€“61).\n",
    "6. Avula, S., Choi, B., & Arguello, J. (2022). The effects of system initiative during conversational interactive shop/retail search. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1), 1â€“30.\n",
    "7. Avula, S., Choi, B., & Arguello, J. (2023). Why and When: Understanding System Initiative during Conversational interactive shop/retail search. arXiv preprint arXiv:2303.13484.\n",
    "8. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877â€“1901.\n",
    "9. Liu, X., Lai, H., Yu, H., Xu, Y., Zeng, A., Du, Z., Zhang, P., Dong, Y., & Tang, J. (2023). WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences. arXiv preprint arXiv:2306.07906.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c57ef04d63031a45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "47ae90b4a353096c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-19T21:19:10.590990Z"
    }
   },
   "id": "d1f5bd4a899a7945"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
