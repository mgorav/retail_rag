{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a203cf",
   "metadata": {},
   "source": [
    "\n",
    "# Smart Retail Navigator: Unifying RAG, LLM, and Annoy for Advanced Query Intelligence\n",
    "\n",
    "This notebook presents an enhanced analysis using a Structured Retrieval-Augmented Generation (RAG) System, specifically tailored for the retail sector. The system leverages advanced data processing techniques and machine learning models to provide comprehensive insights into retail operations, customer behavior, and sales performance. Through detailed examples and explanations, we aim to demonstrate the application of cutting-edge AI technologies in transforming retail analytics and decision-making processes. The below diagram shows overall architecture:\n",
    "\n",
    "![Architecture](architecture.png)\n",
    "\n",
    "The key aspects of the Smart Retail Navigator system architecture:\n",
    "\n",
    "- **Data Layer**: Manages storage and access of retail data \n",
    "\n",
    "- **Retrieval-Augmented Generation (RAG)**: Retrieves relevant data and generates query responses by combining information retrieval and deep learning\n",
    "\n",
    "- **Large Language Models (LLM)**: Understand queries and generate human-like responses after specialized fine-tuning \n",
    "\n",
    "- **Annoy**: Rapidly retrieves most relevant information for queries via similarity searches in vector spaces\n",
    "\n",
    "- **Query Processor**: Coordinates overall workflow - query understanding by LLM, data retrieval via Annoy and RAG, and response generation\n",
    "\n",
    "- **Analytics Module**: Transforms system outputs into business insights for data-driven decision making\n",
    "\n",
    "The architecture strategically integrates the latest innovations in AI to ensure scalability, efficiency, accuracy and cutting-edge capabilities for enabling advanced retail analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e621bb9",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation and Exploration\n",
    "\n",
    "In this section, we delve into the initial steps of our analysis: preparing and exploring the dataset. Our focus is on understanding the characteristics of the data, identifying patterns, and preparing it for further analysis. We'll cover data loading, cleaning, and basic exploratory data analysis (EDA) techniques that are crucial for any data science project.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b13876",
   "metadata": {},
   "source": [
    "\n",
    "## Predictive Modeling and Analysis\n",
    "\n",
    "Following data preparation, we transition to the core of our analysisâ€”predictive modeling. This section explores the creation and evaluation of models that predict future retail trends based on historical data. We'll discuss model selection, training, and validation, emphasizing the importance of accuracy and reliability in predictions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c87bd",
   "metadata": {},
   "source": [
    "\n",
    "## Insights and Conclusion\n",
    "\n",
    "In the final section, we synthesize our findings into actionable insights. Drawing from the data exploration and predictive modeling phases, we outline key takeaways and recommend strategies for retail businesses to optimize their operations, enhance customer satisfaction, and boost sales. This comprehensive analysis demonstrates the transformative potential of AI in retail.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15a15f538ea5c4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:37:01.303344Z",
     "start_time": "2024-02-19T22:37:01.283391Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install transformers annoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326c255dbc81696",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Retrieval:** The similarity between a query \\(q\\) and a document \\(d\\) is often computed using cosine similarity:\n",
    "  \\[ S(d, q) = \\frac{d \\cdot q}{\\|d\\| \\|q\\|} \\]\n",
    "- **Generation:** The probability of generating a word \\(w_t\\) given the context \\(C\\) and previous words \\(w_{<t}\\) is modeled as:\n",
    "  \\[ P(w_t | w_{<t}, C) = \\text{softmax}(W_h h_t) \\]\n",
    "\n",
    "### Large Language Models (LLM)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Self-Attention:** The self-attention mechanism's computation is defined as:\n",
    "  \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
    "\n",
    "### LangChain\n",
    "\n",
    "**Core Concepts:**\n",
    "LangChain, developed as an open-source toolkit, simplifies the creation of applications leveraging LLMs by facilitating integration with external computation and data sources. Its components include:\n",
    "\n",
    "- **Schema:** Defines core data structures like Text, ChatMessages, Examples, and Document, essential for interacting with language models.\n",
    "- **Models:** Categorizes into Language Models, Chat Models, and Text Embedding Models, offering interfaces for seamless integration with LLMs.\n",
    "- **Prompts:** Crafting prompts is vital for directing LLMs. LangChain introduces PromptTemplates for constructing prompts dynamically.\n",
    "- **Indexes:** Serve as bridges between documents/data and LLMs, crucial for enriching models with context-specific information.\n",
    "- **Memory:** Facilitates storing and retrieving chat history or conversational context, enhancing the model's ability to produce coherent and context-aware responses.\n",
    "- **Chains:** Enable the sequencing of multiple components, such as data retrieval, prompt generation, and response parsing, into a cohesive workflow.\n",
    "- **Agents:** Utilize LLMs for selecting and sequencing actions, embodying the decision-making prowess of LLMs in application scenarios.\n",
    "\n",
    "**Innovations in LangChain:**\n",
    "LangChain's modular abstractions for these components significantly lower the barrier to integrating complex LLM functionalities into applications. By abstracting the complexities involved in handling language models, data retrieval, and processing, LangChain empowers developers to build more sophisticated, context-aware applications with ease.\n",
    "\n",
    "**Mathematical Extensions:**\n",
    "While LangChain primarily focuses on the architectural and integration aspects of using LLMs, the underlying mathematical principles of its components (especially Models and Indexes) are grounded in the computations of neural networks, vector space models, and embeddings. For instance, the Text Embedding Models convert textual data into numerical vectors, capturing semantic meanings in a high-dimensional space, which can be mathematically represented as:\n",
    "\\[ \\text{Embedding}(text) = V \\]\n",
    "where \\(V\\) is a vector representing the text in a semantic vector space.\n",
    "\n",
    "### Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
    "\n",
    "**Mathematical Principles:**\n",
    "- **Tree Construction:** Annoy uses random projection trees for partitioning data, optimizing for both speed and memory efficiency.\n",
    "- **Approximate Search:** The search in Annoy is approximated to quickly retrieve near neighbors without exhaustive search, significantly reducing query time.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "This comprehensive overview, enriched with details from the LangChain components guide and mathematical principles, offers a deeper understanding of the technologies and methodologies driving today's AI applications. LangChain, by abstracting the complexity of integrating LLMs with external data and computational resources, stands out as a pivotal framework for developers aiming to leverage the power of language models in their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a73509e88c59e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mock Data Generation\n",
    "\n",
    "The provided Python function `generate_retail_mock_data` is designed to create mock retail product descriptions. It's a versatile tool for generating a dataset that could be used in various retail or e-commerce data analysis projects, machine learning models, or simply for testing and demonstration purposes. Here's a detailed breakdown of how this function works and its components:\n",
    "\n",
    "### Function Definition and Parameters\n",
    "- **Function Name:** `generate_retail_mock_data`\n",
    "- **Parameters:**\n",
    "  - `categories`: A list of strings representing product categories. Default categories are 'shoes', 'apparel', and 'jackets'.\n",
    "  - `num_items_per_category`: An integer that defines how many items to generate per category. The default is 500.\n",
    "  - `seed`: An integer used to initialize the random number generator, ensuring reproducibility of the generated descriptions. The default is 42.\n",
    "  - `enhance_description`: A boolean flag to indicate whether to enhance a subset of product descriptions using the GPT-2 model. The default is `False`.\n",
    "  - `enhanced_samples`: An integer indicating how many product descriptions to enhance with additional creative sentences. The default is 100.\n",
    "\n",
    "### Core Functionality\n",
    "1. **Initializing the Random Seed:** Ensures reproducibility by initializing the random number generator with a specified seed.\n",
    "\n",
    "2. **Base Description Generation:** Iterates over each category to generate product descriptions, including a unique identifier and a combination of three randomly selected features (e.g., 'lightweight', 'durable').\n",
    "\n",
    "3. **Optional Description Enhancement:** For a limited number of products (defined by `enhanced_samples`), the function uses the GPT-2 model to generate and append an additional creative sentence, enhancing the base description.\n",
    "\n",
    "4. **Compilation of Descriptions:** All generated (and optionally enhanced) descriptions are compiled into a list, `product_descriptions`, which is then returned by the function.\n",
    "\n",
    "### Output\n",
    "- Returns a list, `product_descriptions`, containing all generated (and optionally enhanced) product descriptions.\n",
    "\n",
    "### Example Output\n",
    "An example of the function's output, showing a variety of product descriptions, some of which may be enhanced with creative sentences generated by the GPT-2 model:\n",
    "```\n",
    "Shoes 1 with features: durable stylish comfortable in category shoes.\n",
    "Apparel 1 with features: lightweight waterproof durable in category apparel. \"This apparel combines fashion with function.\"\n",
    "Jackets 1 with features: comfortable stylish waterproof in category jackets.\n",
    "...\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "This mock data generation function is invaluable for:\n",
    "- **Development and Testing:** Providing test data for e-commerce platforms to evaluate search and recommendation systems.\n",
    "- **Data Analysis Projects:** Offering a basis for demonstrations or educational projects focused on retail data analytics.\n",
    "- **Machine Learning Models:** Supplying initial training or testing data for models when actual product data is not available.\n",
    "\n",
    "### Customization and Extension\n",
    "The function offers flexibility for customization:\n",
    "- Adjusting product categories, features, or the number of items to better fit specific needs.\n",
    "- Toggling the description enhancement feature or changing the number of enhanced descriptions.\n",
    "\n",
    "This function demonstrates an effective approach for generating rich, varied, and reproducible mock data for retail-related applications, adaptable to a wide array of use cases in the retail and e-commerce sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4263dd944f97f76d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:44:45.801178Z",
     "start_time": "2024-02-19T22:44:33.318239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date\n",
      "Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter\n",
      "Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2\n",
      "Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "import random\n",
    "\n",
    "# Initialize the GPT-2 text generation model\n",
    "model_name = \"gpt2\"\n",
    "text_generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "def clean_description(description):\n",
    "    \"\"\"\n",
    "    Cleans the description by removing HTML tags, URLs, special characters,\n",
    "    and other non-descriptive elements using regex.\n",
    "    \"\"\"\n",
    "    clean_text = re.sub(r'<[^>]+>', '', description)\n",
    "    clean_text = re.sub(r'http[s]?://\\S+', '', clean_text)\n",
    "    clean_text = re.sub(r'[^A-Za-z0-9.,\\'\\s]+', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\[.*?\\]', '', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "def generate_product_description(product_name, features):\n",
    "    prompt = f\"Describe {product_name}, a product featuring {', '.join(features)}:\"\n",
    "    generated_text = text_generator(prompt, max_length=60, num_return_sequences=1)[0]['generated_text']\n",
    "    cleaned_description = clean_description(generated_text)\n",
    "    return cleaned_description\n",
    "\n",
    "def generate_retail_mock_data(product_names, num_items_per_category=10, seed=42):\n",
    "    random.seed(seed)\n",
    "    product_descriptions = []\n",
    "\n",
    "    for category, names in product_names.items():\n",
    "        product_list = random.sample(names, min(num_items_per_category, len(names)))\n",
    "        for product_name in product_list:\n",
    "            features = random.sample(['lightweight', 'durable', 'waterproof', 'stylish', 'comfortable'], 3)\n",
    "            description = generate_product_description(product_name, features)\n",
    "            full_description = f\"{product_name} is a {category} featuring {', '.join(features)}. {description}\"\n",
    "            product_descriptions.append(full_description)\n",
    "\n",
    "    return product_descriptions\n",
    "\n",
    "# Example usage with all Nike shoes\n",
    "nike_shoes = {\n",
    "    'shoes': ['Nike Air Zoom Pegasus', 'Nike Air Max', 'Nike React Infinity Run', 'Nike Free RN']\n",
    "}\n",
    "\n",
    "mock_retail_product_descriptions = generate_retail_mock_data(nike_shoes)  # Reduced for demonstration\n",
    "print(\"\\n\".join(mock_retail_product_descriptions[:15]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9b3ec1860473171",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:44:50.512978Z",
     "start_time": "2024-02-19T22:44:50.496258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy index is built with 4 items.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "vector_length = 100  # Assuming a 100-dimensional vector for each product description\n",
    "annoy_index = AnnoyIndex(vector_length, 'angular')\n",
    "\n",
    "# Mock function to convert descriptions to vectors\n",
    "def description_to_vector(description):\n",
    "    np.random.seed(hash(description) % (2**32 - 1))\n",
    "    return np.random.rand(vector_length)\n",
    "\n",
    "# Adding items to Annoy index\n",
    "for i, description in enumerate(mock_retail_product_descriptions):\n",
    "    vec = description_to_vector(description)\n",
    "    annoy_index.add_item(i, vec)\n",
    "\n",
    "annoy_index.build(10)  # Using 10 trees\n",
    "num_products = len(mock_retail_product_descriptions)\n",
    "print(\"Annoy index is built with\", num_products, \"items.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60101a8acb5c7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
    "\n",
    "Annoy is a C++ library with Python bindings designed to efficiently search for points in space that are close to a given query point, emphasizing speed and minimal memory usage while accepting a trade-off in precision. It's particularly useful for implementing recommendation systems, enhancing search engines, and facilitating various machine learning applications where quick nearest neighbor queries are essential.\n",
    "\n",
    "### Key Features:\n",
    "- **Memory-efficient Indexing:** Annoy creates large, read-only, file-based data structures that are memory-mapped, allowing multiple processes to share the same data without duplicating it in memory.\n",
    "- **Incremental Updates:** It supports adding items to the index incrementally without the need to rebuild the index from scratch, making it suitable for dynamic datasets.\n",
    "- **Persistence:** Annoy indexes can be saved to disk and later reloaded, facilitating persistent storage and retrieval of vector data across sessions.\n",
    "\n",
    "### Usage in This Notebook:\n",
    "In the context of this notebook, Annoy is utilized to store and query embeddings of retail product descriptions. Here's how we integrate Annoy for a practical use-case:\n",
    "\n",
    "1. **Embedding Storage:** We generate embeddings for each product description using a mock function that simulates the conversion of textual descriptions into 100-dimensional vectors. These embeddings are stored in an Annoy index.\n",
    "2. **Index Creation:** An AnnoyIndex instance is created with the specified vector length and metric ('angular' for cosine similarity). Each product's embedding vector is added to the index, which is then built with a specified number of trees to optimize query performance.\n",
    "3. **Querying:** For a given query, its embedding is calculated and used to fetch the nearest neighbors from the Annoy index. This process efficiently identifies the most relevant product descriptions based on the similarity of their embeddings to the query.\n",
    "\n",
    "This demonstration showcases the use of Annoy as a vector database for embedding-based retrieval, illustrating its application in scenarios where finding similar items quickly is crucial, such as in retail product recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29016635cb7d53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Integration of LangChain with RAG and Annoy\n",
    "\n",
    "In this demonstration, we explore the integration of LangChain with Retrieval-Augmented Generation (RAG) and Annoy, showcasing a seamless workflow that combines the retrieval capabilities of Annoy with the generative prowess of a Large Language Model (LLM) provided by Hugging Face's `transformers`. This integration exemplifies how to leverage the strengths of both retrieval and generation to enhance the relevance and richness of generated text based on a given query.\n",
    "\n",
    "### Workflow Overview:\n",
    "1. **Query Embedding:** For a given query, we first convert it into an embedding vector using a mock function `description_to_vector`. This function simulates the process of transforming textual data into a numerical representation that can be processed by machine learning models.\n",
    "\n",
    "2. **Retrieval with Annoy:** Using the query's vector representation, we retrieve the nearest neighbor product descriptions from an Annoy index. Annoy efficiently identifies the vectors in the dataset that are closest to the query vector, based on cosine similarity. This step highlights Annoy's utility in quickly fetching relevant data from a large dataset.\n",
    "\n",
    "3. **Context Assembly:** The retrieved product descriptions are concatenated to form a context string. This assembled context is then used to inform the generation process, ensuring that the generated text is relevant to the specifics of the query.\n",
    "\n",
    "4. **Text Generation with LangChain and RAG:** The concatenated context is fed into a generative model from the `transformers` library, specifically `distilgpt2`, alongside the original query. This generative step uses the context to produce a response that is not only contextually aware but also creatively enriched by the language model.\n",
    "\n",
    "5. **HTML Presentation:** The query, retrieved context, and generated response are presented in an HTML format for enhanced readability. This step demonstrates how the integration of these technologies can be used to create user-friendly outputs for applications such as product recommendation systems or automated customer service responses.\n",
    "\n",
    "### Example Usage:\n",
    "In our example, we query for \"Looking for stylish and comfortable shoes.\" The process involves embedding the query, retrieving related product descriptions using Annoy, and generating a response that synthesizes the query intent with the retrieved information. The final output is displayed in a styled HTML block, making it easy to visualize the integration's effectiveness in producing relevant and engaging content.\n",
    "\n",
    "This integration exemplifies a practical application of combining retrieval and generative models to enhance the capabilities of AI-driven systems. By leveraging the specific strengths of Annoy for efficient data retrieval and the generative capabilities of language models like `distilgpt2`, developers can create sophisticated solutions that address complex queries with contextually rich and relevant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5a1d6191e5a7116",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:45:00.650243Z",
     "start_time": "2024-02-19T22:44:58.760489Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/gauravmalhotra/Documents/MyGithub/retail_in_rag/rag_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n        <h2>Query</h2>\n        <p>Looking for stylish and comfortable shoes</p>\n        <h2>Retrieved Context</h2>\n        <p>Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2 Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date</p>\n        <h2>Response</h2>\n        <p style=\"color: #27ae60;\">Query: Looking for stylish and comfortable shoes. Based on: Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2 Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date project</p>\n    </div>\n    "
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import HTML\n",
    "\n",
    "def simple_langchain_integration_v2(query):\n",
    "    generator = pipeline('text-generation', model='distilgpt2')\n",
    "    # Assuming description_to_vector and other necessary parts are defined elsewhere\n",
    "    query_vector = description_to_vector(query)\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(query_vector, 5)  # Assuming annoy_index is defined elsewhere\n",
    "    retrieved_docs = [mock_retail_product_descriptions[i] for i in nearest_ids]  # Assuming mock_retail_product_descriptions is defined elsewhere\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "    # Added truncation=True to avoid the warning\n",
    "    response = generator(f'Query: {query}. Based on: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)\n",
    "    generated_text = response[0]['generated_text']\n",
    "\n",
    "    # Create HTML content with CSS styling for better visual presentation\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "        <h2>Query</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Retrieved Context</h2>\n",
    "        <p>{retrieved_context}</p>\n",
    "        <h2>Response</h2>\n",
    "        <p style=\"color: #27ae60;\">{generated_text}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example use\n",
    "simple_langchain_integration_v2(\"Looking for stylish and comfortable shoes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266d5d8",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive demonstration of integrating Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) from Hugging Face, leveraging Annoy for efficient nearest neighbor searches, all facilitated through the LangChain framework. The primary focus was on a retail context, showcasing how the amalgamation of retrieval systems and generative models can significantly enhance application capabilities in delivering relevant and contextually rich responses to user queries. Here's a summary of the workflow and key takeaways:\n",
    "\n",
    "- **What:** The notebook outlines the process of generating mock retail product descriptions, indexing these descriptions using Annoy for fast retrieval, and then integrating this with a generative model from Hugging Face (distilGPT-2) for text generation, all orchestrated using the LangChain framework.\n",
    "\n",
    "- **Why:** The integration aims to demonstrate the power of combining vector space retrieval with generative AI to improve the relevance and specificity of responses in a simulated retail query scenario. This approach exemplifies how businesses can leverage AI to enhance user experiences through personalized and context-aware interactions.\n",
    "\n",
    "- **How:**\n",
    "  - **Mock Data Generation:** We started by generating a dataset of mock product descriptions to simulate a retail inventory.\n",
    "  - **Annoy Indexing:** The descriptions were then converted into vector embeddings and indexed using Annoy, enabling efficient similarity-based retrieval.\n",
    "  - **LangChain Integration:** We showcased how LangChain can facilitate the integration of Annoy with a generative LLM to process user queries, retrieve contextually relevant product descriptions, and generate informative responses.\n",
    "  - **Text Generation and Display:** Utilizing the Hugging Face `pipeline` for text generation, the notebook demonstrated generating responses based on the context provided by Annoy's nearest neighbor search, with the output presented in an HTML format for enhanced readability.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Efficiency and Relevance:** The use of Annoy for embedding storage and retrieval showcases an efficient method to add contextual relevance to LLM-generated responses.\n",
    "- **Scalability:** This approach illustrates how scalable solutions can be built for real-world applications, accommodating large datasets typical in retail environments.\n",
    "- **Customizability:** The modular nature of LangChain, combined with the flexibility of Annoy and the generative power of LLMs, underscores the potential for customization according to specific application needs or domains.\n",
    "- **User Experience Enhancement:** The integration exemplifies how AI can be used to significantly enhance user experience, providing a foundation for developing advanced AI-driven retail applications.\n",
    "\n",
    "Through this integration, we've demonstrated a scalable and efficient method to bring context-awareness and relevance to AI-generated text, paving the way for innovative applications in retail and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0d60b19d9e54d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Detailed System Architecture Overview\n",
    "\n",
    "This section delves into the architecture of the system implemented in this notebook, leveraging the Mermaid syntax for a comprehensive visual depiction. We intricately integrate Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) from Hugging Face, employing the Annoy library for efficient similarity searches, all orchestrated within the LangChain framework to enhance retail query processing.\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "![RAG Sequence Diagram](rag_sequence_digram.png)\n",
    "\n",
    "### Architecture Components and Interactions\n",
    "\n",
    "- **User Query:** Initiates the process, where users enter their queries regarding retail products, starting the interaction flow.\n",
    "\n",
    "- **LangChain Framework:** Serves as the central orchestrator, efficiently managing the workflow from user query input to fetching nearest neighbors via Annoy and generating responses through LLM. It ensures seamless integration and communication between different components.\n",
    "\n",
    "- **Annoy Index:** A critical component that stores pre-processed vector embeddings of product descriptions, enabling quick and efficient retrieval of items similar to the user query.\n",
    "\n",
    "- **Retrieve Nearest Neighbors:** This step is crucial for identifying contextually relevant product descriptions based on the user's query, which are then utilized to inform the response generation process.\n",
    "\n",
    "- **LLM (Hugging Face):** At this stage, the system leverages a pre-trained language model to generate detailed and contextually enriched responses by incorporating both the original query and the context provided by the nearest neighbors.\n",
    "\n",
    "- **Generate Response:** The synthesis of input and retrieved context through the LLM culminates in the generation of a response tailored to the user's query, embodying the integration of RAG principles.\n",
    "\n",
    "- **Display Results:** The final step where the system presents the generated response back to the user, completing the cycle and providing a cohesive answer to the query.\n",
    "\n",
    "### Highlighted Features:\n",
    "\n",
    "- **Rapid Context Retrieval:** Utilizes Annoy for the swift retrieval of relevant context, significantly speeding up the response generation process.\n",
    "\n",
    "- **Scalable System Design:** Engineered to accommodate the extensive and growing datasets typical in retail, ensuring the system's scalability.\n",
    "\n",
    "- **Modular and Flexible:** The architecture's modular design promotes flexibility, allowing for the easy replacement or enhancement of individual components, such as swapping the LLM model or modifying the retrieval strategy.\n",
    "\n",
    "- **Enhanced User Engagement:** By delivering precise, context-aware responses, the system significantly improves user interaction and satisfaction, showcasing the potential of AI in transforming retail experiences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bbf53842bc35dc0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:45:25.954918Z",
     "start_time": "2024-02-19T22:45:25.917688Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf7700d",
   "metadata": {},
   "source": [
    "# Enhancing E-commerce with NingLab eCeLLM-S\n",
    "\n",
    "The integration of NingLab's eCeLLM-S model into our retail analytics framework signifies a leap forward in applying advanced Large Language Models (LLMs) to the e-commerce domain. This section delves into the technical underpinnings, objectives, and implementation strategy of leveraging eCeLLM-S for enriching e-commerce content and customer interaction.\n",
    "\n",
    "## Background on LLMs and eCeLLM-S\n",
    "\n",
    "Large Language Models (LLMs) like GPT-3 have revolutionized the field of natural language processing (NLP) with their ability to generate coherent and contextually relevant text across a broad spectrum of applications. Building on this foundation, eCeLLM-S is designed to specifically address the nuances and complexities of e-commerce text generation, from product descriptions to customer engagement.\n",
    "\n",
    "### Core Technologies\n",
    "\n",
    "eCeLLM-S employs a transformer-based architecture, renowned for its self-attention mechanism, which allows the model to weigh the importance of different parts of the input text differently. This is crucial for generating text that is not only grammatically correct but also contextually aligned with the subject matter.\n",
    "\n",
    "#### Mathematical Foundation of Transformers\n",
    "\n",
    "The transformer model utilizes several key equations, including the self-attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "- $Q$, $K$, and $V$ represent the query, key, and value vectors, respectively.\n",
    "- $d_k$ is the dimensionality of the key vectors, providing a normalization factor.\n",
    "\n",
    "This formula allows the model to dynamically focus on different parts of the input sequence, enhancing the relevance and coherence of the generated text.\n",
    "\n",
    "### Objectives of eCeLLM-S Integration\n",
    "\n",
    "- **Contextual Relevance**: Generate text that reflects the specific context of retail products and customer interactions.\n",
    "- **Enhanced Descriptions**: Create detailed and engaging product descriptions that capture the essence and appeal of products.\n",
    "- **Customer Engagement**: Facilitate improved customer interaction through personalized and informative responses.\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "The effective deployment of eCeLLM-S within our retail analysis ecosystem involves a multi-faceted approach:\n",
    "\n",
    "1. **Data Curation**: Assemble a diverse dataset that encompasses a wide range of our retail products and customer feedback.\n",
    "2. **Model Fine-Tuning**: Adapt eCeLLM-S to our specific retail context to enhance its output's relevance and accuracy.\n",
    "3. **Workflow Integration**: Seamlessly incorporate eCeLLM-S-generated content into our product listings and customer service processes.\n",
    "4. **Iterative Refinement**: Continuously monitor and refine the model's performance based on user feedback and emerging retail trends.\n",
    "\n",
    "\n",
    "**The strategic integration of NingLab's eCeLLM-S model represents a forward-thinking approach to harnessing the power of LLMs in the retail sector. By leveraging its advanced text generation capabilities, we can significantly enhance the quality of our product descriptions and the effectiveness of our customer interactions, setting a new benchmark for AI-driven e-commerce excellence.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cdddfc93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T22:46:32.912860Z",
     "start_time": "2024-02-19T22:45:27.631455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05c2f1b73b764197be27a919bbd5e109"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated product description:\n",
      "Generate a catchy product description for a pair of eco-friendly sneakers.\n",
      "Input: \n",
      "Output: Step into the future with these eco-friendly sneakers. Made from recycled materials and organic cotton, these sneakers are not only stylish and comfortable, but also sustainable and ethical. Whether you're running, walking, or dancing, these sneakers will keep you on your feet and on your mission to save the planet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the eCeLLM-S pipeline for text generation\n",
    "eCeLLM_S_pipeline = pipeline(\"text-generation\", model=\"NingLab/eCeLLM-S\")\n",
    "\n",
    "# Example usage with a prompt related to retail\n",
    "prompt = \"Generate a catchy product description for a pair of eco-friendly sneakers\"\n",
    "generated_text = eCeLLM_S_pipeline(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated product description:\")\n",
    "print(generated_text[0]['generated_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33273fd9",
   "metadata": {},
   "source": [
    "\n",
    "### Application in Retail Analysis\n",
    "\n",
    "The above example demonstrates the use of eCeLLM-S in generating a product description. Such capabilities can be extended to various aspects of retail analysis, including but not limited to, generating creative product names, detailed product descriptions, and personalized marketing messages.\n",
    "\n",
    "By integrating eCeLLM-S, we can enhance the customer experience through more engaging and informative content, contributing to improved customer satisfaction and sales.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90eee7",
   "metadata": {},
   "source": [
    "\n",
    "## Integration with Lanchain Using Vector RAG\n",
    "\n",
    "To further enhance our retail analytics capabilities, we integrate Lanchain with vector Retrieval-Augmented Generation (RAG) for advanced query understanding and information retrieval. This integration aims to leverage the synergies between Lanchain's blockchain technology and RAG's powerful retrieval capabilities to enhance data veracity and retrieval efficiency in our retail analysis.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- **Enhance Data Veracity**: Utilize Lanchain to ensure the integrity and veracity of the retail data used in our analysis.\n",
    "- **Improve Retrieval Efficiency**: Leverage vector RAG for efficient retrieval of relevant information from our extensive retail dataset.\n",
    "- **Innovate Retail Analytics**: Combine blockchain technology and state-of-the-art NLP to pioneer innovative retail analytics solutions.\n",
    "\n",
    "Explanation of the below code:\n",
    "\n",
    "- Import required modules:\n",
    "  - `transformers.pipeline`: Importing the pipeline class from the transformers library to easily use pre-trained models.\n",
    "  - `IPython.display.HTML`: Importing the HTML class from the IPython.display module to display HTML content in Jupyter Notebooks or IPython environments.\n",
    "\n",
    "- Define the function `enhanced_langchain_integration(query)`:\n",
    "  - Integrates language model with retail data to generate enhanced responses to queries.\n",
    "  - Uses two different models:\n",
    "    - `eceLLM_generator`: Specifically designed for e-commerce (eCeLLM-S).\n",
    "    - `general_generator`: General-purpose text generation model (DistilGPT-2).\n",
    "  - Converts the input query to a vector representation (assuming a function `description_to_vector` is defined elsewhere).\n",
    "  - Retrieves similar product descriptions based on the query vector using an Annoy index (`annoy_index`).\n",
    "  - Combines retrieved context to provide better context for generation.\n",
    "  - Generates responses using both models based on the query and retrieved context.\n",
    "  - Chooses the final response based on some criteria (e.g., length, relevance).\n",
    "  - Constructs HTML content with CSS styling for visual presentation, including query, retrieved context, and generated response.\n",
    "\n",
    "- Example usage:\n",
    "  - Demonstrates how to use the `enhanced_langchain_integration` function with an example query (\"Looking for stylish and comfortable shoe\").\n",
    "  - Displays the HTML content in the notebook, presenting the query, retrieved context, and generated response with appropriate styling.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a38f316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T22:47:12.858753Z",
     "start_time": "2024-02-19T22:46:52.839178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b44548fed1794c8f8013badd69eb4fbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/gauravmalhotra/Documents/MyGithub/retail_in_rag/rag_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n        <h2>Query</h2>\n        <p>Looking for stylish and comfortable shoe</p>\n        <h2>Retrieved Context</h2>\n        <p>Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2 Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date</p>\n        <h2>Generated Response</h2>\n        <p style=\"color: #27ae60;\">Query: Looking for stylish and comfortable shoe. Based on: Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2 Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date with</p>\n    </div>\n    "
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Function to integrate language model with retail data\n",
    "def enhanced_langchain_integration(query):\n",
    "    # Using eCeLLM-S for e-commerce specific generation and DistilGPT-2 for general text\n",
    "    eceLLM_generator = pipeline('text-generation', model='NingLab/eCeLLM-S')  # Adjust the model ID as needed\n",
    "    general_generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "    # Convert query to vector (Assuming this function is defined and works with your data)\n",
    "    query_vector = description_to_vector(query)\n",
    "\n",
    "    # Retrieve similar product descriptions based on the query vector\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(query_vector, 5)  # Assuming annoy_index is properly set up\n",
    "    retrieved_docs = [mock_retail_product_descriptions[i] for i in nearest_ids]  # Assuming this variable holds your mock data\n",
    "\n",
    "    # Combine retrieved context for better generation\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "\n",
    "    # Generate response with both models and choose based on context\n",
    "    eceLLM_response = eceLLM_generator(f'Query: {query}. Based on: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
    "    general_response = general_generator(f'Query: {query}. Context: {retrieved_context}', max_length=100, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
    "\n",
    "    # Choose response based on some criteria (e.g., length, relevance, etc.) - for simplicity, just using eCeLLM-S here\n",
    "    final_response = eceLLM_response if len(eceLLM_response) > len(general_response) else general_response\n",
    "\n",
    "    # Create HTML content with CSS styling for visual presentation\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "        <h2>Query</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Retrieved Context</h2>\n",
    "        <p>{retrieved_context}</p>\n",
    "        <h2>Generated Response</h2>\n",
    "        <p style=\"color: #27ae60;\">{final_response}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example use\n",
    "enhanced_langchain_integration(\"Looking for stylish and comfortable shoe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "Integrating Langchain with vector RAG requires careful planning and execution, including ensuring data security, optimizing retrieval algorithms, and effectively managing blockchain transactions. The combination of Lanchain's blockchain technology with RAG's NLP capabilities offers a unique opportunity to redefine retail analytics, making it more secure, efficient, and insightful.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The integration of Lanchain with vector RAG represents a significant advancement in retail analytics, offering unprecedented levels of data integrity, retrieval efficiency, and analytical depth. By harnessing these technologies, we can unlock new insights and value from our retail data, driving innovation and excellence in our business operations.\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e47d7387aeb21df4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "#### **Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures the frequency of a word in a document. TF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in a document } d}{\\text{Total number of terms in the document } d}\n",
    "$$\n",
    "\n",
    "#### **Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is within the entire corpus. Words that appear frequently in one document but less frequently in the corpus receive a higher weighting. IDF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } D}{\\text{Number of documents with term } t \\text{ in it}} + 1 \\right)\n",
    "$$\n",
    "\n",
    "Note: Adding 1 in the denominator prevents division by zero for terms that appear in all documents.\n",
    "\n",
    "#### **TF-IDF Calculation**\n",
    "\n",
    "The TF-IDF value is calculated by multiplying TF and IDF:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "This results in a matrix where each row represents a document and each column represents a term in the corpus, with values indicating the significance of each term to the document.\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a machine learning algorithm for dimensionality reduction well-suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "#### **Mathematical Foundation**\n",
    "\n",
    "Given a set of points in a high-dimensional space, t-SNE first computes probabilities \\(p_{ij}\\) that are proportional to the similarity of objects \\(x_i\\) and \\(x_j\\), as follows:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
    "$$\n",
    "\n",
    "In the low-dimensional space, t-SNE computes similar probabilities \\(q_{ij}\\) using a Student-t distribution:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "The Kullback-Leibler divergence between the distributions \\(P\\) and \\(Q\\) is minimized by gradient descent:\n",
    "\n",
    "$$\n",
    "C = \\text{KL}(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "#### **Perplexity**\n",
    "\n",
    "Perplexity is a parameter for t-SNE that suggests how to balance attention between local and global aspects of your data and is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = 2^{H(P_i)}\n",
    "$$\n",
    "\n",
    "where \\(H(P_i)\\) is the Shannon entropy of \\(P_i\\) measured in bits.\n",
    "\n",
    "### Application in the Notebook Context\n",
    "\n",
    "In the context of the notebook, TF-IDF is used to transform mock retail product descriptions into numerical vectors, capturing the importance of terms within descriptions relative to their frequency across all descriptions. These vectors serve as features that represent each product's textual information.\n",
    "\n",
    "t-SNE is then applied to these TF-IDF vectors to reduce the high-dimensional feature space to a 2D space suitable for visualization. The `perplexity` parameter is dynamically adjusted based on the dataset size to ensure meaningful dimensionality reduction.\n",
    "\n",
    "The final visualization with seaborn plots the t-SNE-transformed points, colored by product categories, to illustrate the distribution and clustering of products based on their descriptions. This process highlights the power of combining TF-IDF for feature extraction with t-SNE for visualization, providing insights into the semantic relationships within the dataset.\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "#### **Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures the frequency of a word in a document. TF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in a document } d}{\\text{Total number of terms in the document } d}\n",
    "$$\n",
    "\n",
    "#### **Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is within the entire corpus. Words that appear frequently in one document but less frequently in the corpus receive a higher weighting. IDF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } D}{\\text{Number of documents with term } t \\text{ in it}} + 1 \\right)\n",
    "$$\n",
    "\n",
    "Note: Adding 1 in the denominator prevents division by zero for terms that appear in all documents.\n",
    "\n",
    "#### **TF-IDF Calculation**\n",
    "\n",
    "The TF-IDF value is calculated by multiplying TF and IDF:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "This results in a matrix where each row represents a document and each column represents a term in the corpus, with values indicating the significance of each term to the document.\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a machine learning algorithm for dimensionality reduction well-suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "#### **Mathematical Foundation**\n",
    "\n",
    "Given a set of points in a high-dimensional space, t-SNE first computes probabilities \\(p_{ij}\\) that are proportional to the similarity of objects \\(x_i\\) and \\(x_j\\), as follows:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
    "$$\n",
    "\n",
    "In the low-dimensional space, t-SNE computes similar probabilities \\(q_{ij}\\) using a Student-t distribution:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "The Kullback-Leibler divergence between the distributions \\(P\\) and \\(Q\\) is minimized by gradient descent:\n",
    "\n",
    "$$\n",
    "C = \\text{KL}(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "#### **Perplexity**\n",
    "\n",
    "Perplexity is a parameter for t-SNE that suggests how to balance attention between local and global aspects of your data and is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = 2^{H(P_i)}\n",
    "$$\n",
    "\n",
    "where \\(H(P_i)\\) is the Shannon entropy of \\(P_i\\) measured in bits.\n",
    "\n",
    "### Application in the Notebook Context\n",
    "\n",
    "In the context of the notebook, TF-IDF is used to transform mock retail product descriptions into numerical vectors, capturing the importance of terms within descriptions relative to their frequency across all descriptions. These vectors serve as features that represent each product's textual information.\n",
    "\n",
    "t-SNE is then applied to these TF-IDF vectors to reduce the high-dimensional feature space to a 2D space suitable for visualization. The `perplexity` parameter is dynamically adjusted based on the dataset size to ensure meaningful dimensionality reduction.\n",
    "\n",
    "The final visualization with seaborn plots the t-SNE-transformed points, colored by product categories, to illustrate the distribution and clustering of products based on their descriptions. This process highlights the power of combining TF-IDF for feature extraction with t-SNE for visualization, providing insights into the semantic relationships within the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28666c8fb07d81b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Combine the descriptions into a single string per product for TF-IDF\n",
    "descriptions = [desc.split(\" with features: \")[0] for desc in mock_retail_product_descriptions]\n",
    "\n",
    "# Use TF-IDF to simulate embeddings\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_embeddings = vectorizer.fit_transform(descriptions).toarray()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:52:48.474845Z",
     "start_time": "2024-02-19T22:52:48.457841Z"
    }
   },
   "id": "b329373da4c7b86c",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 21.459835,  20.480707],\n       [ 94.14081 ,  23.338009],\n       [ 95.783394, -49.451584],\n       [ 23.169798, -52.17629 ]], dtype=float32)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Adjust perplexity to be less than the number of samples\n",
    "# Assuming 'tfidf_embeddings' contains your dataset's embeddings\n",
    "n_samples = tfidf_embeddings.shape[0]\n",
    "perplexity_value = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "tsne_results = tsne.fit_transform(tfidf_embeddings)\n",
    "\n",
    "tsne_results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:52:49.718201Z",
     "start_time": "2024-02-19T22:52:49.464837Z"
    }
   },
   "id": "41bb8e862fade2cf",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "               Metric  \\\n0  Pairwise Distances   \n1   Similarity Matrix   \n2          Perplexity   \n\n                                                                                                                                                                                                                                                              Value  \\\n0  [[0.0, 72.73711395263672, 102.05154418945312, 72.67711639404297], [72.73711395263672, 0.0, 72.8081283569336, 103.63056182861328], [102.05154418945312, 72.8081283569336, 0.0, 72.6646957397461], [72.67711639404297, 103.63056182861328, 72.6646957397461, 0.0]]   \n1                                                                                                                                                                          [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]   \n2                                                                                                                                                                                                                                                               1.0   \n\n                                                                                                             Analysis  \n0        Pairwise distances between points in the t-SNE embedding space. High values indicate more dissimilar points.  \n1             Measure of similarity between data points in the embedding space. High values indicate high similarity.  \n2  A measure of how well the probability distribution predicts a sample. Higher values indicate a smoother embedding.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n      <th>Value</th>\n      <th>Analysis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pairwise Distances</td>\n      <td>[[0.0, 72.73711395263672, 102.05154418945312, 72.67711639404297], [72.73711395263672, 0.0, 72.8081283569336, 103.63056182861328], [102.05154418945312, 72.8081283569336, 0.0, 72.6646957397461], [72.67711639404297, 103.63056182861328, 72.6646957397461, 0.0]]</td>\n      <td>Pairwise distances between points in the t-SNE embedding space. High values indicate more dissimilar points.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Similarity Matrix</td>\n      <td>[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]</td>\n      <td>Measure of similarity between data points in the embedding space. High values indicate high similarity.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Perplexity</td>\n      <td>1.0</td>\n      <td>A measure of how well the probability distribution predicts a sample. Higher values indicate a smoother embedding.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Function to compute pairwise distances\n",
    "def compute_pairwise_distances(X):\n",
    "    distances = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(i+1, X.shape[0]):\n",
    "            distances[i, j] = np.linalg.norm(X[i] - X[j])\n",
    "            distances[j, i] = distances[i, j]  # Distance matrix is symmetric\n",
    "    return distances\n",
    "\n",
    "# Function to compute similarity matrix\n",
    "def compute_similarity_matrix(distances, sigma):\n",
    "    similarities = np.exp(-distances**2 / (2 * sigma**2))\n",
    "    np.fill_diagonal(similarities, 0)  # Diagonal elements are set to 0\n",
    "    return similarities\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(P):\n",
    "    epsilon = 1e-12  # Small epsilon value to avoid numerical issues\n",
    "    P = np.maximum(P, epsilon)  # Replace zero values with epsilon\n",
    "    entropy = -np.sum(P * np.log2(P))\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n",
    "\n",
    "# Combine the descriptions into a single string per product for TF-IDF\n",
    "descriptions = [desc.split(\" with features: \")[0] for desc in mock_retail_product_descriptions]\n",
    "\n",
    "# Use TF-IDF to simulate embeddings\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_embeddings = vectorizer.fit_transform(descriptions).toarray()\n",
    "\n",
    "# Adjust perplexity to be less than the number of samples\n",
    "n_samples = tfidf_embeddings.shape[0]\n",
    "perplexity_value = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "\n",
    "# Perform t-SNE embedding\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "tsne_results = tsne.fit_transform(tfidf_embeddings)\n",
    "\n",
    "# Compute pairwise distances\n",
    "pairwise_distances = compute_pairwise_distances(tsne_results)\n",
    "\n",
    "# Compute similarity matrix\n",
    "sigma = 1.0  # Adjust sigma according to your data\n",
    "similarity_matrix = compute_similarity_matrix(pairwise_distances, sigma)\n",
    "\n",
    "# Compute perplexity\n",
    "perplexity = compute_perplexity(similarity_matrix)\n",
    "\n",
    "# Create a DataFrame to summarize the computed values and provide analysis\n",
    "data = {\n",
    "    \"Metric\": [\"Pairwise Distances\", \"Similarity Matrix\", \"Perplexity\"],\n",
    "    \"Value\": [pairwise_distances, similarity_matrix, perplexity],\n",
    "    \"Analysis\": [\n",
    "        \"Pairwise distances between points in the t-SNE embedding space. High values indicate more dissimilar points.\",\n",
    "        \"Measure of similarity between data points in the embedding space. High values indicate high similarity.\",\n",
    "        \"A measure of how well the probability distribution predicts a sample. Higher values indicate a smoother embedding.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set option to display complete DataFrame content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the DataFrame\n",
    "display(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:52:50.831043Z",
     "start_time": "2024-02-19T22:52:50.675686Z"
    }
   },
   "id": "784dc1fb79319453",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date',\n 'Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter',\n 'Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2',\n \"Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called\"]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mock_retail_product_descriptions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T22:55:24.347077Z",
     "start_time": "2024-02-19T22:55:24.312690Z"
    }
   },
   "id": "c7459af3d14c02f2",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Nike Air Zoom Pegasus is a shoes featuring durable, comfortable, waterproof. Describe Nike Air Zoom Pegasus, a product featuring durable, comfortable, waterproof A single, durable mesh cushion with a molded surface for an air dry look Designed for outdoor travel Designed for comfortable walking in the sun High res to date',\n 'Nike Free RN is a shoes featuring lightweight, comfortable, waterproof. Describe Nike Free RN, a product featuring lightweight, comfortable, waterproof a shoe with great traction, and great durability, the Nylon Nylon M Nylon. The innovative design features Nylon Nylon bonded mesh when wearing a M Isole, the mesh increases in diameter',\n 'Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2',\n \"Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called\"]"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_retail_product_descriptions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:21:02.439230Z",
     "start_time": "2024-02-19T23:21:02.420216Z"
    }
   },
   "id": "d326b3dc4d6fcecc",
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.manifold import TSNE\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from transformers import pipeline\n",
    "# \n",
    "# # Function to compute pairwise distances\n",
    "# def compute_pairwise_distances(X):\n",
    "#     distances = np.zeros((X.shape[0], X.shape[0]))\n",
    "#     for i in range(X.shape[0]):\n",
    "#         for j in range(i+1, X.shape[0]):\n",
    "#             distances[i, j] = np.linalg.norm(X[i] - X[j])\n",
    "#             distances[j, i] = distances[i, j]  # Distance matrix is symmetric\n",
    "#     return distances\n",
    "# \n",
    "# # Function to compute similarity matrix\n",
    "# def compute_similarity_matrix(distances, sigma):\n",
    "#     similarities = np.exp(-distances**2 / (2 * sigma**2))\n",
    "#     np.fill_diagonal(similarities, 0)  # Diagonal elements are set to 0\n",
    "#     return similarities\n",
    "# \n",
    "# # Function to compute perplexity\n",
    "# def compute_perplexity(P):\n",
    "#     epsilon = 1e-12  # Small epsilon value to avoid numerical issues\n",
    "#     P = np.maximum(P, epsilon)  # Replace zero values with epsilon\n",
    "#     entropy = -np.sum(P * np.log2(P))\n",
    "#     perplexity = 2 ** entropy\n",
    "#     return perplexity\n",
    "# \n",
    "# # Combine the descriptions into a single string per product for TF-IDF\n",
    "# descriptions = [desc.split(\" with features: \")[0] for desc in mock_retail_product_descriptions]\n",
    "# \n",
    "# # Use TF-IDF to simulate embeddings\n",
    "# vectorizer = TfidfVectorizer(max_features=100)\n",
    "# tfidf_embeddings = vectorizer.fit_transform(descriptions).toarray()\n",
    "# \n",
    "# # Adjust perplexity to be less than the number of samples\n",
    "# n_samples = tfidf_embeddings.shape[0]\n",
    "# perplexity_value = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "# \n",
    "# # Perform t-SNE embedding\n",
    "# tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(tfidf_embeddings)\n",
    "# \n",
    "# # Compute pairwise distances\n",
    "# pairwise_distances = compute_pairwise_distances(tsne_results)\n",
    "# \n",
    "# # Compute similarity matrix\n",
    "# sigma = 1.0  # Adjust sigma according to your data\n",
    "# similarity_matrix = compute_similarity_matrix(pairwise_distances, sigma)\n",
    "# \n",
    "# # Compute perplexity\n",
    "# perplexity = compute_perplexity(similarity_matrix)\n",
    "# print(\"Perplexity:\", perplexity)\n",
    "# \n",
    "# # Plot t-SNE results\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x=tsne_results[:,0], y=tsne_results[:,1], hue=categories, palette=\"viridis\")\n",
    "# plt.title('t-SNE Visualization of Mock Product Descriptions')\n",
    "# plt.xlabel('t-SNE Dimension 1')\n",
    "# plt.ylabel('t-SNE Dimension 2')\n",
    "# plt.legend(title='Category')\n",
    "# plt.show()\n",
    "# \n",
    "# # Prescriptive Analysis\n",
    "# generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "# prescriptive_analysis = \"Based on the t-SNE visualization, we can observe that products in the 'apparel' category tend to cluster together in the lower left region of the plot. This suggests that these products share similar characteristics or features. To capitalize on this insight, the retail company could create targeted marketing campaigns or promotions specifically tailored to customers interested in apparel products. Additionally, the company could consider expanding its apparel product line or introducing new apparel-related offerings to meet the demand demonstrated by the clustering pattern.\"\n",
    "# recommendation = generator(prescriptive_analysis, max_length=100, do_sample=False)[0]['generated_text']\n",
    "# print(\"Prescriptive Analysis Recommendation:\", recommendation)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:24:48.969844Z",
     "start_time": "2024-02-19T23:24:48.938657Z"
    }
   },
   "id": "aeefac8fb9e9d4ce",
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collaborative Query Handling in a Retail Environment\n",
    "This example groups queries by potential customer intent, useful for addressing multiple user queries in a retail context."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3178880ebf1a09f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id                                  query predicted_category\n0        1           sustainable outdoor clothing           clothing\n1        2  eco-friendly water bottles for hiking        accessories\n2        3                   durable hiking boots           footwear",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>query</th>\n      <th>predicted_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>sustainable outdoor clothing</td>\n      <td>clothing</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>eco-friendly water bottles for hiking</td>\n      <td>accessories</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>durable hiking boots</td>\n      <td>footwear</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the zero-shot classification pipeline with a suitable model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define your user queries\n",
    "user_queries = [\n",
    "    {\"user_id\": 1, \"query\": \"sustainable outdoor clothing\"},\n",
    "    {\"user_id\": 2, \"query\": \"eco-friendly water bottles for hiking\"},\n",
    "    {\"user_id\": 3, \"query\": \"durable hiking boots\"}\n",
    "]\n",
    "\n",
    "# Candidate categories for classification\n",
    "categories = [\"clothing\", \"accessories\", \"footwear\"]\n",
    "\n",
    "# Classify each query and add the predicted category to the query dictionary\n",
    "for query in user_queries:\n",
    "    classification = classifier(query[\"query\"], categories, multi_label=False)\n",
    "    query[\"predicted_category\"] = classification[\"labels\"][0]  # Assign the most probable category\n",
    "\n",
    "# Convert the enriched user queries into a pandas DataFrame for display\n",
    "df_queries = pd.DataFrame(user_queries)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_queries)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:17:28.678371Z",
     "start_time": "2024-02-19T23:17:20.292266Z"
    }
   },
   "id": "47ae90b4a353096c",
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query Processing and Clarification\n",
    "Using a hypothetical model to rewrite queries for better search results, assuming the availability of a model that can process and rewrite queries:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e24ace820461724"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: best running shoes\n",
      "Rewritten Query: best running shoes. I think it all means more about getting the best shoes.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def rewrite_query(query):\n",
    "    # Initialize the text-generation pipeline with an accessible model\n",
    "    generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "    # Generate rewritten query\n",
    "    # Note: You might need to adjust max_length according to your specific requirements\n",
    "    results = generator(query, max_length=60, num_return_sequences=1)\n",
    "    rewritten = results[0]['generated_text'].strip()\n",
    "\n",
    "    return rewritten\n",
    "\n",
    "# Example query to be rewritten\n",
    "original_query = \"best running shoes\"\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "\n",
    "print(f\"Original Query: {original_query}\")\n",
    "print(f\"Rewritten Query: {rewritten_query}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:17:30.591644Z",
     "start_time": "2024-02-19T23:17:28.681697Z"
    }
   },
   "id": "d1f5bd4a899a7945",
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmented Search Results Presentation\n",
    "Example of enriching search results with insights, assuming search results are retrieved from a dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "897a34b350434a46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming mock_retail_product_descriptions is a list of descriptions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert descriptions to vectors\n",
    "product_vectors = [description_to_vector(desc) for desc in mock_retail_product_descriptions]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_products = pd.DataFrame({\n",
    "    'description': mock_retail_product_descriptions,\n",
    "    'vector': product_vectors\n",
    "})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:17:30.607455Z",
     "start_time": "2024-02-19T23:17:30.602046Z"
    }
   },
   "id": "7b542ca4bf822659",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Products for Query: 'Eco-friendly water bottle'\n",
      "----------------------------------\n",
      "1. Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called\n",
      "2. Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Initialize Annoy Index\n",
    "# Assuming we have a DataFrame 'df_products' with product vectors in a column named \"vector\"\n",
    "vector_dimension = len(df_products[\"vector\"].iloc[0])  # Dimension of the product vectors\n",
    "annoy_index = AnnoyIndex(vector_dimension, 'angular')  # Using 'angular' distance metric\n",
    "\n",
    "# Step 2: Populate Annoy Index with Product Vectors\n",
    "for product_id, vector in enumerate(df_products[\"vector\"]):\n",
    "    annoy_index.add_item(product_id, vector)\n",
    "\n",
    "annoy_index.build(10)  # Building the index with 10 trees for efficient querying\n",
    "\n",
    "# Step 3: Define Function to Convert Search Queries to Vectors\n",
    "# This is a placeholder function. In practice, use a model to generate vectors similar to product vectors.\n",
    "def query_to_vector(query):\n",
    "    # Example: Random vector generation (Replace with actual vectorization logic)\n",
    "    return np.random.rand(vector_dimension)\n",
    "\n",
    "# Step 4: Define Function to Find Similar Products Using Annoy\n",
    "def find_similar_products_annoy(query_vector, num_results=2):\n",
    "    # Fetch indices of similar products from the Annoy index\n",
    "    similar_product_ids = annoy_index.get_nns_by_vector(query_vector, num_results, include_distances=False)\n",
    "    # Return DataFrame rows corresponding to similar products\n",
    "    return df_products.iloc[similar_product_ids]\n",
    "\n",
    "# Step 5: Augment Search Results with Additional Logic\n",
    "def augment_search_results_with_annoy(query):\n",
    "    # Convert the query to a vector (adapt this to use actual query vectorization)\n",
    "    query_vector = query_to_vector(query)\n",
    "    # Find similar products based on the query vector\n",
    "    similar_products = find_similar_products_annoy(query_vector)\n",
    "    # Return the descriptions of similar products (customize this as needed)\n",
    "    return similar_products[\"description\"].tolist()\n",
    "\n",
    "# Demonstration of Usage\n",
    "query = \"Eco-friendly water bottle\"\n",
    "augmented_results = augment_search_results_with_annoy(query)\n",
    "\n",
    "# Print the descriptions of similar products\n",
    "print(\"Similar Products for Query: '{}'\".format(query))\n",
    "print(\"----------------------------------\")\n",
    "for idx, description in enumerate(augmented_results, 1):\n",
    "    print(\"{}. {}\".format(idx, description))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:17:30.679771Z",
     "start_time": "2024-02-19T23:17:30.611026Z"
    }
   },
   "id": "c1ddb6880d10ee52",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div style=\"display: flex; justify-content: space-between;\">\n        <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n            <h2>Query:</h2>\n            <p>Eco-friendly water shoes</p>\n        </div>\n        <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n            <h2>Similar Products:</h2>\n            <ul>\n    <li>1. Nike Air Max is a shoes featuring stylish, lightweight, comfortable. Describe Nike Air Max, a product featuring stylish, lightweight, comfortable Inside the Nike Air Max you will find the following items 2 pairs of Nike Air Max T Shirts and 3 pairs of Nike Air Max T Shirt Worn 2</li><li>2. Nike React Infinity Run is a shoes featuring lightweight, durable, comfortable. Describe Nike React Infinity Run, a product featuring lightweight, durable, comfortable you'll need a pair of running shoes with the option to wear them out in no time within 15 minutes. Nike also introduced a special edition Fit or Die shoe set, one of which is called</li>\n            </ul>\n        </div>\n    </div>\n    "
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Step 1: Initialize Annoy Index\n",
    "# Assuming we have a DataFrame 'df_products' with product vectors in a column named \"vector\"\n",
    "vector_dimension = len(df_products[\"vector\"].iloc[0])  # Dimension of the product vectors\n",
    "annoy_index = AnnoyIndex(vector_dimension, 'angular')  # Using 'angular' distance metric\n",
    "\n",
    "# Step 2: Populate Annoy Index with Product Vectors\n",
    "for product_id, vector in enumerate(df_products[\"vector\"]):\n",
    "    annoy_index.add_item(product_id, vector)\n",
    "\n",
    "annoy_index.build(10)  # Building the index with 10 trees for efficient querying\n",
    "\n",
    "# Step 3: Define Function to Convert Search Queries to Vectors\n",
    "# This is a placeholder function. In practice, use a model to generate vectors similar to product vectors.\n",
    "def query_to_vector(query):\n",
    "    # Example: Random vector generation (Replace with actual vectorization logic)\n",
    "    return np.random.rand(vector_dimension)\n",
    "\n",
    "# Step 4: Define Function to Find Similar Products Using Annoy\n",
    "def find_similar_products_annoy(query_vector, num_results=2):\n",
    "    # Fetch indices of similar products from the Annoy index\n",
    "    similar_product_ids = annoy_index.get_nns_by_vector(query_vector, num_results, include_distances=False)\n",
    "    # Return DataFrame rows corresponding to similar products\n",
    "    return df_products.iloc[similar_product_ids]\n",
    "\n",
    "# Step 5: Augment Search Results with Additional Logic\n",
    "def augment_search_results_with_annoy(query):\n",
    "    # Convert the query to a vector (adapt this to use actual query vectorization)\n",
    "    query_vector = query_to_vector(query)\n",
    "    # Find similar products based on the query vector\n",
    "    similar_products = find_similar_products_annoy(query_vector)\n",
    "    # Return the descriptions of similar products (customize this as needed)\n",
    "    return similar_products[\"description\"].tolist()\n",
    "\n",
    "# Function to create visually appealing output\n",
    "def create_visual_output(query, augmented_results):\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "        <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "            <h2>Query:</h2>\n",
    "            <p>{query}</p>\n",
    "        </div>\n",
    "        <div style=\"border: 2px solid #2c3e50; border-radius: 10px; padding: 20px;\">\n",
    "            <h2>Similar Products:</h2>\n",
    "            <ul>\n",
    "    \"\"\"\n",
    "    for idx, description in enumerate(augmented_results, 1):\n",
    "        html_content += f\"<li>{idx}. {description}</li>\"\n",
    "    html_content += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html_content)\n",
    "\n",
    "# Demonstration of Usage\n",
    "query = \"Eco-friendly water shoes\"\n",
    "augmented_results = augment_search_results_with_annoy(query)\n",
    "\n",
    "# Create visually appealing output\n",
    "create_visual_output(query, augmented_results)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T23:19:06.709699Z",
     "start_time": "2024-02-19T23:19:06.685584Z"
    }
   },
   "id": "db51d9ddec02df13",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bc3d89f122b3d551"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
